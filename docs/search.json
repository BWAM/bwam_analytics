[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BWAM Analytics",
    "section": "",
    "text": "Welcome\nThe goals of this quarto book are to:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Landing Page</span>"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "BWAM Analytics",
    "section": "",
    "text": "Document the where the Bureau of Water Assessment and Management (BWAM) data are stored and how it is structured\nDescribe how to access BWAM data\nProvide examples of how to query and perform analytics on BWAM data\nDescribe how to maintain BWAM data",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Landing Page</span>"
    ]
  },
  {
    "objectID": "index.html#issues-and-edits",
    "href": "index.html#issues-and-edits",
    "title": "BWAM Analytics",
    "section": "0.1 Issues and Edits",
    "text": "0.1 Issues and Edits\nIf you identify an issue with the content and/or you would like to edit a page of this book, please select the “Edit this page” or “Report an issue” text at the top right of the page.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Landing Page</span>"
    ]
  },
  {
    "objectID": "wqma_oracle_db.html",
    "href": "wqma_oracle_db.html",
    "title": "\n2  WQMA Oracle Data Warehouse\n",
    "section": "",
    "text": "2.1 Connecting to the WQMA Data Warehouse\nIf you have not already, please setup an ODBC connection to the WQMA data warehouse following the instructions in the ODBC section.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>WQMA Oracle Data Warehouse</span>"
    ]
  },
  {
    "objectID": "wqma_oracle_db.html#connecting-to-the-wqma-data-warehouse",
    "href": "wqma_oracle_db.html#connecting-to-the-wqma-data-warehouse",
    "title": "\n2  WQMA Oracle Data Warehouse\n",
    "section": "",
    "text": "2.1.1 Install nexus\nThe nexus package is available on GitHub. To install packages from GitHub, first install the package, pak.\n\ninstall.packages(\"pak\")\n\nUse pak to install the nexus package from GitHub. The code below specifies the organization (“BWAM”) and the repository (“nexus”). If this fails, try the next code chunk.\n\npak::pak(\"BWAM/nexus\")\n\nIf pak fails to install the package, try using devtools and gitcreds. The difficulty with this option is you must install a GitHub PAT. To setup a PAT, see Managing Git(Hub) Credentials.\n\ndevtools::install_github(\n  repo = \"BWAM/nexus\",\n  auth_token = gitcreds::gitcreds_get()$password)\n\n\n2.1.2 Get Connected\nTo work with the data warehouse, you must first connect to it. This is very similar to specifying specifying a file directory containing multiple CSV files you want to import into R. nexus provides the function, get_connected(), to simplify the process for connecting to the data warehouse. At a minimum, you must provide your data warehouse username. You will be prompted to enter your password in a pop-up window. You should never supply your password in plain text because it increases the chances you accidentally share your password with others. For example, it is very easy to accidentally share your password when committing and pushing other changes to GitHub.\n\n\nWhen you are finished querying the data warehouse, please use\n`DBI::dbDisconnect(conn = insert-your-connection-name-here)`.\nThis message is displayed once per session.\n\n\n\ncon &lt;- nexus::get_connected(username = \"ZSMITH\")\n\n\n2.1.3 Understanding the Data Warehouse Structure\nA primary goal of the WQMA Data Warehouse and nexus is to allow users to focus on the task at hand and NOT have to spend much energy cleaning data or learning how tables relate to one another. This section outlines a number of functions that can be used to rapidly learn about the data warehouse and it’s contents.\n\n2.1.3.1 Tables and Columns and Definitions, Oh My!\nThe DBI function, dbListTables(), can be used to get a vector of tables in a database. For the WQMA data warehouse, we also need to specify the name of the schema we want to explore.\n\nDBI::dbListTables(con, schema_name = \"WQMA_OWNER\")\n\n [1] \"TAXONOMY_REFERENCE_JUNCTION\" \"PARAMETER\"                  \n [3] \"SITE\"                        \"TAXONOMIC_TRAIT\"            \n [5] \"SAMPLE_DELIVERY_GROUP\"       \"BASIN\"                      \n [7] \"RESULT\"                      \"ACCESSIONS_TABLES\"          \n [9] \"ACCESSIONS_DATA_MAP\"         \"PREVIOUS_TAXON_NAME\"        \n[11] \"QUALITY_CONTROL\"             \"PROJECT\"                    \n[13] \"PREVIOUS_SITE_ID\"            \"ACCESSIONS_FLD_XFM\"         \n[15] \"EVENT\"                       \"SAMPLE\"                     \n[17] \"WATERBODY\"                   \"ACCESSIONS_CDE_MAP\"         \n[19] \"TAXONOMY_REFERENCE\"          \"ACCESSIONS\"                 \n[21] \"RESULT_QUALIFIER\"            \"TAXONOMIC_ABUNDANCE\"        \n[23] \"PARAMETER_NAME\"              \"TAXONOMY\"                   \n[25] \"ACCESSIONS_KEY_MAP\"         \n\n\nHowever, it is recommended that you use the nexus get_data_dictionary() function because it provides much more information than dbListTables(). Setting the argument just_tables = TRUE will return tibble (i.e., a fancy data frame) of all tables in the data warehouse and their definitions.\n\n(dictionary &lt;- nexus::get_data_dictionary(just_tables = TRUE))\n\n# A tibble: 19 × 2\n   table_name                  definition                                       \n   &lt;chr&gt;                       &lt;chr&gt;                                            \n 1 BASIN                       This table represents the 17 major drainage basi…\n 2 EVENT                       A table representing all distinct sampling event…\n 3 PARAMETER                   This table represents the distinct parameters pr…\n 4 PARAMETER_NAME              This table provides descriptions of the paramete…\n 5 PREVIOUS_SITE_ID            This table represents site IDs that were previou…\n 6 PREVIOUS_TAXON_NAME         This table represents taxonomic names that were …\n 7 PROJECT                     This table represents the monitoring projects th…\n 8 QUALITY_CONTROL             This table contains quality control results asso…\n 9 RESULT                      This table represents numeric and/or categorical…\n10 RESULT_QUALIFIER            The table provides a detailed description of res…\n11 SAMPLE                      This table represents distinct samples collected…\n12 SAMPLE_DELIVERY_GROUP       A table containing all valid Sample Delivery Gro…\n13 SITE                        This table represents the distinct monitoring lo…\n14 TAXONOMIC_ABUNDANCE         This table contains taxonomic abundances— the co…\n15 TAXONOMIC_TRAIT             This table represents taxonomic traits, such as …\n16 TAXONOMY                    This table represents the tax's observed in New …\n17 TAXONOMY_REFERENCE          This table identifies taxonomic reference materi…\n18 TAXONOMY_REFERENCE_JUNCTION This is a junction table to facilitate the relat…\n19 WATERBODY                   This table represents waterbodies in New York St…\n\n\nRunning get_data_dictionary() with all the default arguments will return all table and column definitions in the data warehouse.\n\n(dictionary &lt;- nexus::get_data_dictionary())\n\n# A tibble: 218 × 11\n   table_name column_name    examples   definition  db_data_type character_limit\n   &lt;chr&gt;      &lt;chr&gt;          &lt;list&gt;     &lt;chr&gt;       &lt;chr&gt;                  &lt;dbl&gt;\n 1 BASIN      BASIN          &lt;chr [17]&gt; A unique 2… NUMBER                    NA\n 2 BASIN      BASIN_NAME     &lt;chr [17]&gt; The names … VARCHAR2(50)              50\n 3 BASIN      &lt;NA&gt;           &lt;NULL&gt;     This table… &lt;NA&gt;                      NA\n 4 EVENT      EVENT_DATETIME &lt;chr [4]&gt;  The date a… DATETIME                  NA\n 5 EVENT      EVENT_ID       &lt;chr [10]&gt; A unique s… VARCHAR2(50)              50\n 6 EVENT      PROJECT        &lt;chr [10]&gt; A monitori… VARCHAR2(10…             100\n 7 EVENT      QAPP_ID        &lt;chr [5]&gt;  The Qualit… VARCHAR2(50)              50\n 8 EVENT      SITE_ID        &lt;dbl [10]&gt; A unique s… NUMBER                    NA\n 9 EVENT      &lt;NA&gt;           &lt;NULL&gt;     A table re… &lt;NA&gt;                      NA\n10 PARAMETER  CASRN          &lt;chr [10]&gt; A CAS Regi… VARCHAR2(50)              50\n# ℹ 208 more rows\n# ℹ 5 more variables: not_null &lt;lgl&gt;, surrogate_key &lt;lgl&gt;, primary_key &lt;chr&gt;,\n#   foreign_key &lt;chr&gt;, parent_table &lt;chr&gt;\n\n\nThe dictionary contains an examples field that provides examples of the type of data contained in each column. If there are less than 20 unique values in a column, all possible values are shown. For example, you can see all of the possible values for WATERBODY_TYPE using the following code:\n\ndictionary |&gt; \n  dplyr::filter(table_name %in% \"WATERBODY\",\n                column_name %in% \"WATERBODY_TYPE\") |&gt; \n  dplyr::pull(examples)\n\n[[1]]\n[1] \"lake\"         \"river_stream\" \"other\"        \"outfall\"     \n\n\nYou can supply a character vector of table names to the argument table_vec to subset the tibble to only the table or tables of interest. The example below, only shows the definitions for the BASIN and WATERBODY tables.\n\n(dictionary &lt;- nexus::get_data_dictionary(table_vec = c(\"BASIN\",\n                                                        \"WATERBODY\")))\n\n# A tibble: 12 × 11\n   table_name column_name       examples definition db_data_type character_limit\n   &lt;chr&gt;      &lt;chr&gt;             &lt;list&gt;   &lt;chr&gt;      &lt;chr&gt;                  &lt;dbl&gt;\n 1 BASIN      BASIN             &lt;chr&gt;    A unique … NUMBER                    NA\n 2 BASIN      BASIN_NAME        &lt;chr&gt;    The names… VARCHAR2(50)              50\n 3 BASIN      &lt;NA&gt;              &lt;NULL&gt;   This tabl… &lt;NA&gt;                      NA\n 4 WATERBODY  WATERBODY_CODE    &lt;chr&gt;    A unique … VARCHAR2(50)              50\n 5 WATERBODY  BEACH_PRESENT     &lt;chr&gt;    Identifie… VARCHAR2(15)              15\n 6 WATERBODY  FISHERIES_INDEX_… &lt;chr&gt;    This is a… VARCHAR2(75)              75\n 7 WATERBODY  PUBLIC_WATER_SUP… &lt;chr&gt;    Identifie… VARCHAR2(15)              15\n 8 WATERBODY  USGS_POND_NUMBER  &lt;chr&gt;    A unique … VARCHAR2(15)              15\n 9 WATERBODY  WATERBODY_ALTERN… &lt;chr&gt;    Some wate… VARCHAR2(10…             100\n10 WATERBODY  WATERBODY_NAME    &lt;chr&gt;    The name … VARCHAR2(10…             100\n11 WATERBODY  WATERBODY_TYPE    &lt;chr&gt;    Inidicate… VARCHAR2(50)              50\n12 WATERBODY  &lt;NA&gt;              &lt;NULL&gt;   This tabl… &lt;NA&gt;                      NA\n# ℹ 5 more variables: not_null &lt;lgl&gt;, surrogate_key &lt;lgl&gt;, primary_key &lt;chr&gt;,\n#   foreign_key &lt;chr&gt;, parent_table &lt;chr&gt;\n\n\nSimilar to table_vec, you can supply a character vector of column names to the argument column_vec to subset the tibble to only the column or columns of interest. This is useful for:\n\nGetting the definition of a column(s) of interest.\nQuickly identifying which table a column comes from.\n\nThe example below shows the definitions for two columns of interest from two different tables.\n\n(dictionary &lt;- nexus::get_data_dictionary(\n  column_vec = c(\"BASIN_NAME\",\n                 \"WATERBODY_NAME\")\n))\n\n# A tibble: 2 × 11\n  table_name column_name    examples   definition   db_data_type character_limit\n  &lt;chr&gt;      &lt;chr&gt;          &lt;list&gt;     &lt;chr&gt;        &lt;chr&gt;                  &lt;dbl&gt;\n1 BASIN      BASIN_NAME     &lt;chr [17]&gt; The names o… VARCHAR2(50)              50\n2 WATERBODY  WATERBODY_NAME &lt;chr [10]&gt; The name of… VARCHAR2(10…             100\n# ℹ 5 more variables: not_null &lt;lgl&gt;, surrogate_key &lt;lgl&gt;, primary_key &lt;chr&gt;,\n#   foreign_key &lt;chr&gt;, parent_table &lt;chr&gt;\n\n\n\n2.1.4 Get Data\n\n2.1.4.1 Single Table Queries\nTo query data from a single table you can use functions from the dplyr and DBI packages. Below I have established a remote connection to the BASIN table in the data warehouse. I have not yet pulled the data into R. You may think of this as a preview of the data that you will pull into R. As you can see, you need to provide a connection to a database, the schema, and the table name.\n\n(basin_tbl &lt;- dplyr::tbl(con,\n                        I(\"WQMA_OWNER.BASIN\")))\n\n# Source:   table&lt;WQMA_OWNER.BASIN&gt; [?? x 6]\n# Database: Oracle 19.00.0000[ZSMITH@WQMAP/]\n   BASIN BASIN_NAME  CREATE_DATE         END_DATE UPDATE_DATE\n   &lt;chr&gt; &lt;chr&gt;       &lt;dttm&gt;              &lt;dttm&gt;   &lt;dttm&gt;     \n 1 01    Lake Erie-… 2024-03-26 13:32:45 NA       NA         \n 2 02    Allegheny … 2024-03-26 13:32:45 NA       NA         \n 3 03    Lake Ontar… 2024-03-26 13:32:45 NA       NA         \n 4 04    Genesee Ri… 2024-03-26 13:32:45 NA       NA         \n 5 05    Chemung Ri… 2024-03-26 13:32:45 NA       NA         \n 6 06    Susquehann… 2024-03-26 13:32:45 NA       NA         \n 7 07    Seneca-One… 2024-03-26 13:32:45 NA       NA         \n 8 08    Black Rive… 2024-03-26 13:32:45 NA       NA         \n 9 09    St. Lawren… 2024-03-26 13:32:45 NA       NA         \n10 10    Lake Champ… 2024-03-26 13:32:45 NA       NA         \n# ℹ more rows\n# ℹ 1 more variable: UPDATED_BY_GUID &lt;chr&gt;\n\n\nTo bring the data into R, you need to use the dplyr function, collect().\n\n(basin_df &lt;- dplyr::collect(basin_tbl))\n\n# A tibble: 17 × 6\n   BASIN BASIN_NAME  CREATE_DATE         END_DATE UPDATE_DATE\n   &lt;chr&gt; &lt;chr&gt;       &lt;dttm&gt;              &lt;dttm&gt;   &lt;dttm&gt;     \n 1 01    Lake Erie-… 2024-03-26 13:32:45 NA       NA         \n 2 02    Allegheny … 2024-03-26 13:32:45 NA       NA         \n 3 03    Lake Ontar… 2024-03-26 13:32:45 NA       NA         \n 4 04    Genesee Ri… 2024-03-26 13:32:45 NA       NA         \n 5 05    Chemung Ri… 2024-03-26 13:32:45 NA       NA         \n 6 06    Susquehann… 2024-03-26 13:32:45 NA       NA         \n 7 07    Seneca-One… 2024-03-26 13:32:45 NA       NA         \n 8 08    Black Rive… 2024-03-26 13:32:45 NA       NA         \n 9 09    St. Lawren… 2024-03-26 13:32:45 NA       NA         \n10 10    Lake Champ… 2024-03-26 13:32:45 NA       NA         \n11 11    Upper Huds… 2024-03-26 13:32:45 NA       NA         \n12 12    Mohawk Riv… 2024-03-26 13:32:45 NA       NA         \n13 13    Lower Huds… 2024-03-26 13:32:45 NA       NA         \n14 14    Delaware R… 2024-03-26 13:32:45 NA       NA         \n15 15    Passaic-Ne… 2024-03-26 13:32:45 NA       NA         \n16 17    Atlantic O… 2024-03-26 13:32:45 NA       NA         \n17 16    Housatonic… 2024-03-26 13:32:45 NA       NA         \n# ℹ 1 more variable: UPDATED_BY_GUID &lt;chr&gt;\n\n\nYou can use dplyr functions to work with the data in the database. dplyr will translate these functions to SQL for you.\nThis example: 1. Uses the basin_tbl connection established before 2. Filters the BASIN column to the rows that represent the major drainage basins “03” and “07” 3. Retains only the BASIN_NAME column 4. Collects the data from the database and into R.\n\n# 1\nbasin_tbl |&gt; \n# 2\n  dplyr::filter(BASIN %in% c(\"03\", \"07\")) |&gt; \n# 3\n  dplyr::select(BASIN_NAME) |&gt; \n# 4\n  dplyr::collect()\n\n# A tibble: 2 × 1\n  BASIN_NAME                      \n  &lt;chr&gt;                           \n1 Lake Ontario & Minor Tribs.     \n2 Seneca-Oneida-Oswego River Basin\n\n\nThis example: 1. Connects to the “EVENT” table 2. filters to the rows were the EVENT_DATETIME column to only the events that occurred between July 1-7th of 2022. 3. Collects the data from the database and into R.\n\n# 1\ndplyr::tbl(con,\n           I(\"WQMA_OWNER.EVENT\")) |&gt; \n# 2\n  dplyr::filter(\n    dplyr::between(\n      x = EVENT_DATETIME,\n      left = to_date(\"2022-07-01\", \"YYYY-MM-DD HH24:MI:SS\"),\n      right = to_date(\"2022-07-07\", \"YYYY-MM-DD HH24:MI:SS\")\n    )\n  ) |&gt; \n# 3\n  dplyr::collect()\n\n# A tibble: 215 × 9\n   EVENT_ID      EVENT_DATETIME      SITE_ID PROJECT QAPP_ID CREATE_DATE        \n   &lt;chr&gt;         &lt;dttm&gt;                &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dttm&gt;             \n 1 13-FDRY-0.4_… 2022-07-06 09:15:00   27578 Undefi… Unknown 2024-03-26 14:26:14\n 2 13-STNY-0.8_… 2022-07-06 15:00:00   28025 Undefi… Unknown 2024-03-26 14:26:14\n 3 13-STNY-0.8_… 2022-07-05 15:00:00   28025 Undefi… Unknown 2024-03-26 14:26:14\n 4 13-GUNK-40.3… 2022-07-05 13:15:00   27611 PWS pr… Unknown 2024-03-26 14:26:11\n 5 13-WKLEI_T4-… 2022-07-05 15:30:00   28182 PWS pr… Unknown 2024-03-26 14:26:11\n 6 14-COUL-0.9_… 2022-07-05 10:00:00   28349 PWS pr… Unknown 2024-03-26 14:26:11\n 7 07-SXML-2.2_… 2022-07-06 11:45:00   25790 PWS pr… Unknown 2024-03-26 14:26:11\n 8 07-SXML-8.0_… 2022-07-06 10:30:00   25791 PWS pr… Unknown 2024-03-26 14:26:11\n 9 07-SXML-12.4… 2022-07-06 09:15:00   25789 PWS pr… Unknown 2024-03-26 14:26:11\n10 13-CRUM-1.5_… 2022-07-06 11:00:00   27545 PWS pr… Unknown 2024-03-26 14:26:11\n# ℹ 205 more rows\n# ℹ 3 more variables: END_DATE &lt;dttm&gt;, UPDATE_DATE &lt;dttm&gt;,\n#   UPDATED_BY_GUID &lt;chr&gt;\n\n\n\n2.1.4.2 Multiple Table Queries (Or Operating on the Data Model)\nIn most situations, it is recommended that you acquire data using the get_data_model() function from nexus. This function has been tailored to connect to establish a remote connection to each table in the data warehouse and define the relationships between tables.\n\n(data_model &lt;- nexus::get_data_model(con = con))\n\n── Table source ────────────────────────────────────────────────────────────────\nsrc:  Oracle 19.00.0000[ZSMITH@WQMAP/]\n── Metadata ────────────────────────────────────────────────────────────────────\nTables: `BASIN`, `EVENT`, `PARAMETER`, `PARAMETER_NAME`, `PREVIOUS_SITE_ID`, … (19 total)\nColumns: 200\nPrimary keys: 13\nForeign keys: 18\n\n\nWe can visualize the table relationships with the get_erd() function.\n\nnexus::get_erd(data_model)\n\n\n\n\n\nIf you want to collect a single table, such as BASIN, you can use the following code. The get_data_model() function hides some of the complexity of connecting to the table through dplyr, such as establishing the schema, and makes it simple to import data into R.\n\n(basin_df2 &lt;- data_model$BASIN |&gt; \n  dplyr::collect())\n\n# A tibble: 17 × 2\n   BASIN BASIN_NAME                      \n   &lt;chr&gt; &lt;chr&gt;                           \n 1 01    Lake Erie-Niagara River Basin   \n 2 02    Allegheny River Basin           \n 3 03    Lake Ontario & Minor Tribs.     \n 4 04    Genesee River Basin             \n 5 05    Chemung River Basin             \n 6 06    Susquehanna River Basin         \n 7 07    Seneca-Oneida-Oswego River Basin\n 8 08    Black River Basin               \n 9 09    St. Lawrence River Basin        \n10 10    Lake Champlain River Basin      \n11 11    Upper Hudson River Basin        \n12 12    Mohawk River Basin              \n13 13    Lower Hudson River Basin        \n14 14    Delaware River Basin            \n15 15    Passaic-Newark (Ramapo River)   \n16 16    Housatonic River Basin          \n17 17    Atlantic Ocean-Long Island Sound\n\n\nOr…\n\n(basin_df2 &lt;- data_model |&gt; \n   dm::pull_tbl(table = \"BASIN\") |&gt; \n  dplyr::collect())\n\n# A tibble: 17 × 2\n   BASIN BASIN_NAME                      \n   &lt;chr&gt; &lt;chr&gt;                           \n 1 01    Lake Erie-Niagara River Basin   \n 2 02    Allegheny River Basin           \n 3 03    Lake Ontario & Minor Tribs.     \n 4 04    Genesee River Basin             \n 5 05    Chemung River Basin             \n 6 06    Susquehanna River Basin         \n 7 07    Seneca-Oneida-Oswego River Basin\n 8 08    Black River Basin               \n 9 09    St. Lawrence River Basin        \n10 10    Lake Champlain River Basin      \n11 11    Upper Hudson River Basin        \n12 12    Mohawk River Basin              \n13 13    Lower Hudson River Basin        \n14 14    Delaware River Basin            \n15 15    Passaic-Newark (Ramapo River)   \n16 16    Housatonic River Basin          \n17 17    Atlantic Ocean-Long Island Sound\n\n\n\n2.1.5 Example Queries\nYou cannot import all data from the WQMA Data Warehouse into the R– the connection will time out or you will crash your R session. However, there is no need to read in all data to R. With DBI, dplyr, dm, and nexus you have the ability to do the following and more within the database:\n\npreview data\nfilter data\nselect only the columns of interest\njoin tables together\n\nYou should try to narrow your focus to the smallest amount of data you need for your task. Queries will return results much faster if you limit the number of rows and columns that need to be transferred from the data warehouse into R. This might mean filtering the rows by basin, waterbody, site, sampling event, project, parameter, etc. or a combination of these factors to get only the data necessary. Similarly, selecting only the tables and columns of interest will help to speed queries up.\nIf you are feeling nervous about not pulling all data in, I hope that it\nnexus is designed to work with the R-package, dm. dm stands for “data model” and it provides a number of useful functions for working with relational data and has great documentation on how to use those functions (see Tutorials, Cheatsheet, and Technical Articles on the dm website). Many of the functions are designed to both execute locally in R and to be translated into a SQL query to be executed by a relational database or data warehouse. When possible, it is recommended that you favor dm queries over dbplyr, DBI, and custom SQL queries. It is not bad to write dbplyr, DBI, and custom SQL queries, but dm provides an elegant syntax for leveraging relational data that would be very difficult and time intensive to re-implement with other tools; in other words, dm is generally more efficient because easier to write and understand.\n\n2.1.5.1 Lake Sites with Phosphorus Data Collected in the last 5-Years\n\nDefine the time period of interest as start_date and end_date\n\nStart with the data_model object defined previously\nFilter the WATERBODY table to only the rows where the column WATERBODY_TYPE is “lake”.\nFilter the PARAMETER table to only the rows where the column PARAMTER_NAME is “phosphorus”.\nFilter the EVENT table to only the rows where the column EVENT_DATETIME represents a date in the last 5-years.\nIn this case, we are only interested in the SITE table. It’s important to note that we can use the data model to perform queries on tables besides the SITE table that ultimately influence the rows of the SITE table returned.\nIn this case, we are only interested in keeping the columns associated with the site identifier, SITE_CODE, and the sites coordinates (LATITUDE and LONGITUDE).\nCollects the data from the database and into R.\n\n\n# 1\nstart_date &lt;- Sys.Date() - lubridate::years(5)\nend_date &lt;- Sys.Date()\n\nsite_remote &lt;- data_model |&gt; # 2\n  dm::dm_filter(\n    WATERBODY = WATERBODY_TYPE == \"lake\", # 3\n    PARAMETER = PARAMETER_NAME == \"phosphorus\", # 4\n  EVENT = dplyr::between( # 5\n    x = EVENT_DATETIME,\n    left = to_date(start_date, \"YYYY-MM-DD HH24:MI:SS\"),\n    right = to_date(end_date, \"YYYY-MM-DD HH24:MI:SS\")\n  )\n  ) |&gt;\n  dm::pull_tbl(SITE) |&gt;  # 6\n  dplyr::select( # 7\n    SITE_CODE,\n    LATITUDE,\n    LONGITUDE\n  )\n\nsite_dm &lt;- dplyr::collect(site_remote) # 8\n\n\n2.1.5.2 Site\n\nsite_remote &lt;- dm::dm_filter(data_model,\n                             SITE = SITE_CODE %in% c(\"07-ONON-1.0\"))\n\n\nsite_dm &lt;- dplyr::collect(site_remote)\n\n\n(single_df &lt;- site_dm |&gt; \n  dm::dm_flatten_to_tbl(.start = RESULT, .recursive = TRUE) |&gt; \n  dplyr::select(\n    EVENT_ID,\n    SITE_CODE,\n    REPLICATE,\n    SAMPLE_METHOD,\n    SAMPLE_METHOD_DESCRIPTION,\n    PARAMETER_NAME,\n    RESULT_VALUE,\n    UNIT,\n    LATITUDE,\n    LONGITUDE,\n    BASIN,\n    BASIN_NAME,\n    WATERBODY_NAME,\n    WATERBODY_TYPE\n  ))\n\n# A tibble: 1,693 × 14\n   EVENT_ID             SITE_CODE REPLICATE SAMPLE_METHOD SAMPLE_METHOD_DESCRI…¹\n   &lt;chr&gt;                &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;                 \n 1 07-ONON-1.0_1986090… 07-ONON-… 1         survey        sample_event_info_sur…\n 2 07-ONON-1.0_1986090… 07-ONON-… 1         multiprobe    unknown               \n 3 07-ONON-1.0_1986090… 07-ONON-… 1         multiprobe    unknown               \n 4 07-ONON-1.0_1986090… 07-ONON-… 1         multiprobe    unknown               \n 5 07-ONON-1.0_1986090… 07-ONON-… 1         multiprobe    unknown               \n 6 07-ONON-1.0_1986090… 07-ONON-… 1         survey        macroinvertebrate_fie…\n 7 07-ONON-1.0_1986090… 07-ONON-… 1         survey        macroinvertebrate_fie…\n 8 07-ONON-1.0_1986090… 07-ONON-… 1         survey        macroinvertebrate_fie…\n 9 07-ONON-1.0_1986090… 07-ONON-… 1         survey        macroinvertebrate_fie…\n10 07-ONON-1.0_1986090… 07-ONON-… 1         survey        macroinvertebrate_fie…\n# ℹ 1,683 more rows\n# ℹ abbreviated name: ¹​SAMPLE_METHOD_DESCRIPTION\n# ℹ 9 more variables: PARAMETER_NAME &lt;chr&gt;, RESULT_VALUE &lt;dbl&gt;, UNIT &lt;chr&gt;,\n#   LATITUDE &lt;dbl&gt;, LONGITUDE &lt;dbl&gt;, BASIN &lt;chr&gt;, BASIN_NAME &lt;chr&gt;,\n#   WATERBODY_NAME &lt;chr&gt;, WATERBODY_TYPE &lt;chr&gt;\n\n\n\n2.1.5.3 Project\n\nproject_remote &lt;- data_model |&gt; \n  dm::dm_filter(PROJECT = PROJECT_TYPE == \"RIBS Routine\") |&gt; \n  dm::dm_select_tbl(\n    WATERBODY,\n    SITE,\n    EVENT,\n    SAMPLE,\n    RESULT,\n    PARAMETER\n  )\n\n\nproject_dm &lt;- dplyr::collect(project_remote)\n\n\n(single_df &lt;- project_dm |&gt; \n  dm::dm_flatten_to_tbl(.start = RESULT, .recursive = TRUE) |&gt; \n  dplyr::select(\n    EVENT_ID,\n    SITE_CODE,\n    REPLICATE,\n    SAMPLE_METHOD,\n    SAMPLE_METHOD_DESCRIPTION,\n    PARAMETER_NAME,\n    RESULT_VALUE,\n    UNIT,\n    LATITUDE,\n    LONGITUDE,\n    BASIN,\n    WATERBODY_NAME,\n    WATERBODY_TYPE\n  ))\n\n# A tibble: 18,911 × 13\n   EVENT_ID             SITE_CODE REPLICATE SAMPLE_METHOD SAMPLE_METHOD_DESCRI…¹\n   &lt;chr&gt;                &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;                 \n 1 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n 2 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n 3 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n 4 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n 5 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n 6 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n 7 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n 8 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n 9 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n10 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n# ℹ 18,901 more rows\n# ℹ abbreviated name: ¹​SAMPLE_METHOD_DESCRIPTION\n# ℹ 8 more variables: PARAMETER_NAME &lt;chr&gt;, RESULT_VALUE &lt;dbl&gt;, UNIT &lt;chr&gt;,\n#   LATITUDE &lt;dbl&gt;, LONGITUDE &lt;dbl&gt;, BASIN &lt;chr&gt;, WATERBODY_NAME &lt;chr&gt;,\n#   WATERBODY_TYPE &lt;chr&gt;\n\n\n\n2.1.5.4 Parameter\n\nchloride_remote &lt;- dm::dm_filter(data_model,\n                             PROJECT = PROJECT_TYPE == \"RIBS Routine\",\n                             PARAMETER = PARAMETER_NAME == \"chloride\")\n\n\nchloride_dm &lt;- dplyr::collect(chloride_remote)\n\n\n(single_df &lt;- chloride_dm |&gt; \n  dm::dm_flatten_to_tbl(.start = RESULT, .recursive = TRUE) |&gt; \n  dplyr::select(\n    EVENT_ID,\n    SITE_CODE,\n    REPLICATE,\n    SAMPLE_METHOD,\n    SAMPLE_METHOD_DESCRIPTION,\n    PARAMETER_NAME,\n    RESULT_VALUE,\n    UNIT,\n    LATITUDE,\n    LONGITUDE,\n    BASIN,\n    BASIN_NAME,\n    WATERBODY_NAME,\n    WATERBODY_TYPE\n  ))\n\n# A tibble: 623 × 14\n   EVENT_ID             SITE_CODE REPLICATE SAMPLE_METHOD SAMPLE_METHOD_DESCRI…¹\n   &lt;chr&gt;                &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;                 \n 1 01-BUFF-1.7_2018041… 01-BUFF-… 1         unknown       unknown               \n 2 01-BUFF-1.7_2018061… 01-BUFF-… 1         unknown       unknown               \n 3 01-BUFF-1.7_2018081… 01-BUFF-… 1         unknown       unknown               \n 4 01-BUFF-1.7_2018101… 01-BUFF-… 1         unknown       unknown               \n 5 01-BUFF-1.7_2019041… 01-BUFF-… 1         unknown       unknown               \n 6 01-BUFF-1.7_2019061… 01-BUFF-… 1         unknown       unknown               \n 7 01-BUFF-1.7_2019081… 01-BUFF-… 1         unknown       unknown               \n 8 01-BUFF-1.7_2019102… 01-BUFF-… 1         unknown       unknown               \n 9 01-BUFF-1.7_2020042… 01-BUFF-… 1         unknown       unknown               \n10 01-BUFF-1.7_2020061… 01-BUFF-… 1         unknown       unknown               \n# ℹ 613 more rows\n# ℹ abbreviated name: ¹​SAMPLE_METHOD_DESCRIPTION\n# ℹ 9 more variables: PARAMETER_NAME &lt;chr&gt;, RESULT_VALUE &lt;dbl&gt;, UNIT &lt;chr&gt;,\n#   LATITUDE &lt;dbl&gt;, LONGITUDE &lt;dbl&gt;, BASIN &lt;chr&gt;, BASIN_NAME &lt;chr&gt;,\n#   WATERBODY_NAME &lt;chr&gt;, WATERBODY_TYPE &lt;chr&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>WQMA Oracle Data Warehouse</span>"
    ]
  },
  {
    "objectID": "wqma_oracle_db.html#setup-oracle-odbc-connectionsec-odbc",
    "href": "wqma_oracle_db.html#setup-oracle-odbc-connectionsec-odbc",
    "title": "\n2  WQMA Oracle Data Warehouse\n",
    "section": "\n2.2 Setup Oracle ODBC Connection{sec-odbc}",
    "text": "2.2 Setup Oracle ODBC Connection{sec-odbc}\n\n2.2.1 Objective\nSetup an ODBC connection to the WQMA Data Warehouse.\n\n2.2.2 What is an ODBC?\nODBC stands for Open Database Connectivity and, once configured accordingly, it allows the user to connect to a database from another program, such as R.\n\n2.2.3 Oracle’s Instant Client ODBC Driver\n\nOracle’s Instant Client ODBC software is a standalone package that offers the full functionality of the Oracle ODBC driver (except the Oracle service for Microsoft Transaction Server) with a simple install. –From: https://www.oracle.com/database/technologies/releasenote-odbc-ic.html\n\nIn other words, Oracle’s Instant Client is a set of software that needs to be installed on your computer in order for you to connect to an Oracle database.\n\n\n\n\n\n\nCaution\n\n\n\nI don’t care what Oracle says, this is anything but simple for the average user.\n\n\n\n\n\n\n\n\nNote\n\n\n\nI was directed to the Oracle Instant Client from Posit’s webpage on connecting to an Oracle database..\n\n\n\n2.2.4 Procedure\n\nVisit Oracle’s Instant Client ODBC Release Notes webpage\n\nFollow the instructions in Installing Oracle Instant Client ODBC/On Windows section. You must first install the Instant Client Basic package and then install the Instant Client ODBC package. I have copied and modified the text from the webpage to try and make the steps easier to follow.\n\n\n“Install the Instant Client Basic or Basic Light package, as described above.”\n\nFollow the instructions at the head of the page in the “Installing Oracle Instant Client Basic and Basic Lite” section, which boils down to Installing Oracle’s Instant Client from here.\nSelect the Instant Client for Microsoft Windows (x64).\nSelect the most recent version of Instant Client for Microsoft Windows and a table will expand.\nDownload the Basic Package to the root of your Downloads folder. You cannot download the file directly to your C-drive (C:\\).\nRight-click on the downloaded zip file in your Downloads folder, select Extract All, browse to your C-drive (C:\\), and select Extract.\n\n\n\n“Download the Instant Client ODBC package. Unzip it in the same directory as your Basic or Basic Light package.”\n\nThe ODBC package is located on the same downloads page as the Basic Package installed in step 2.1 above. If you need to navigate back to this page, follow steps in 2.1.1 - 2.1.3.\nScroll down until you see the ODBC Package– it is under the subheading: Development and Runtime - optional packages.\nInstall the ODBC package to your Downloads folder. You cannot download the file directly to your C-drive (C:\\).\nRight-click on the downloaded zip file in your Downloads folder, select Extract All, browse to your C-drive (C:\\), select the folder you created in step 2.1.5, and select Extract.\n\n\n\nThe C-drive directory, created in step 2.1, must be put on your computers PATH.\n\nIn your Windows Search Bar, look up and open “Edit the system environmental variables.”\nIn the Advanced tab of the pop-up window, click the “Environmental Variables…” button\n\nThis step requires admin-rights.\n\n\nIn the “System variables” section,:\n\nfind the existing “Path” variable\nselect the “Path” row\nclick on the “Edit…” button\n\n\n\nIn the pop-up:\n\nClick on the “New” button\nClick on the “Browse…” button\nSelect the C-drive path to the instant client directory created in step 2.1\n\n\n\n\n\n\n\nThis step requires admin-rights. “Execute odbc_install.exe from the Instant Client directory.”\n\nNavigate to the folder created in step 2.2.4.\nDouble-click the odbc_install.exe.\n\n\n\n\n\nSetup a 64-bit ODBC connection.\n\nIn your Windows search bar, type “ODBC Data Sources”.\nSelect the Option that ends in “64-bit”.\nIn the window that appears, select the “Add” button.\nSelect the Oracle Instant Client option (should look something like this: “Oracle in instant_client_19_19”) and click the “Finish” button. If you do not see the Oracle Instant Client Option, then your ODBC installation was not successful. Please try to install the ODBC again.\n\nConfiguration:\n\nData Source Name: wqma_prod\nDescription: WQMA Data Warehouse Production Version\nTNS Service Name: WQMAP\n\nUser ID: [Enter your database User ID]\n\nExample: JDOE\n\n\n\nClick on the “Test Connection” Button, enter your database password, and click on the “OK” button. If there are no errors, you should receive a message stating “Connection successful.”\n\nIf you are entering a temporary password, you will be prompted for a new password.\n\n\nClick the “OK” button in the Oracle ODBC Driver Configuration window.\n\n\n\n\nCongratulations! You have successfully downloaded, installed, and configured the necessary software to connect to an Oracle database.\n\n2.2.5 Keyring Setup (Optional)\nIf you do not want to enter your password each time you establish a connection, you can setup a keyring to securely store and access your password. Again, you should NEVER type out your password in plain text.\n\nkeyring::key_set(\"your-user-name\",\n                 keyring = \"wqma-example\")\n\n\nkeyring::key_get(\"your-user-name\",\n                 keyring = \"wqma-example\")\n\nOnce the keyring is established, you can point to the keyring in the get_connected() function using the kerying argument and you will not need to provide your password each time you connect to the WQMA data warehouse.\n\ncon &lt;- nexus::get_connected(username = \"your-user-name\",\n                            keyring = \"wqma-example\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>WQMA Oracle Data Warehouse</span>"
    ]
  },
  {
    "objectID": "analytical_data_store.html",
    "href": "analytical_data_store.html",
    "title": "\n3  Analytical Data Store\n",
    "section": "",
    "text": "3.0.1 Just Give Me the Data\nSome example queries provided to get you started quickly…\n# All data\nall_df &lt;- open_dataset(obt_result_dir) |&gt; \n  collect()\n\n# All lake data\nlake_df &lt;- open_dataset(obt_result_dir) |&gt; \n  filter(WATERBODY_TYPE %in% \"lake\") |&gt; \n  collect() \n\n# All TP results\ntp_df &lt;- open_dataset(obt_result_dir) |&gt; \n  filter(PARAMETER_NAME %in% \"phosphorus\",\n         FRACTION %in% \"total\") |&gt;\n  select(SITE_CODE,\n         EVENT_ID,\n         EVENT_DATETIME,\n         FRACTION,\n         PARAMETER_NAME,\n         RESULT_VALUE,\n         UNIT,\n         RESULT_QUALIFIER) |&gt; \n  distinct() |&gt; \n  collect()\n\n# Count of the number of sites with chloride data by waterbody type\ncl_count_df &lt;- open_dataset(obt_result_dir) |&gt; \n  filter(PARAMETER_NAME %in% \"chloride\") |&gt;\n  distinct(WATERBODY_TYPE, SITE_CODE) |&gt; \n  count(WATERBODY_TYPE) |&gt; \n  collect()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analytical Data Store</span>"
    ]
  },
  {
    "objectID": "analytical_data_store.html#parquet-and-arrow-introduction",
    "href": "analytical_data_store.html#parquet-and-arrow-introduction",
    "title": "\n3  Analytical Data Store\n",
    "section": "\n3.1 Parquet and Arrow Introduction",
    "text": "3.1 Parquet and Arrow Introduction\n\nApache Parquet is an open source, column-oriented data file format designed for efficient data storage and retrieval. it provides high performance compression and encoding schemes to handle complex data in bulk and is supported in many programming language and analtyics tools.\n-Parquet (apache.org)\n\nAs described above, parquet files are designed for efficient data retrieval. The data are stored in a columnar format, enabling rapid data retrievals. Using Apache Arrow, analytics can be performed in-memory (using RAM) on parquet files without the need to pull of the data into R or python. In general, this means queries can be performed very rapidly and you only load data you need into R or python.\nAdditionally, parquet files have meta data that describe the column types (character, numeric, date, etc.). Unlike, column separated values (CSVs) or excel file (XLSX), there is no need to tell R or python what the data type of data are stored in each column. This enables users to query data and begin working with it immediately with out the overhead of specifying column type details with every new analysis.\n\n\n\n\n\n\nNote\n\n\n\nFor more information about Apache Parquet or Arrow, please visit:\n\nParquet (apache.org)\nApache Arrow | Apache Arrow",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analytical Data Store</span>"
    ]
  },
  {
    "objectID": "analytical_data_store.html#query-overview",
    "href": "analytical_data_store.html#query-overview",
    "title": "\n3  Analytical Data Store\n",
    "section": "\n3.2 Query Overview",
    "text": "3.2 Query Overview\nParquet files can be queried using the python library, PyArrow, or the R package, arrow. The remainder of this chapter will focus on using the R package, arrow.\nQuerying data is very similar to reading CSV files with read.csv(), but it is a two step process:\n\nConnect to the parquet file using the arrow function, open_dataset()\nRead data into R, using the dplyr function, collect()\n\n\nobt &lt;- open_dataset(obt_result_dir) |&gt; # 1\n  collect() # 2\n\n\n\n\n\n\n\nTip\n\n\n\nAlthough it is not recommended for these data stores, you can use the arrow function, read_parquet(), to perform the above query in a single step. This is not recommended, because in almost all instances you do not need all data in a data store for your analysis.\n\n\nThe open_dataset() function can be followed by dplyr functions to query and perform analytics on the data store before bringing the data into R.\n\n\n\n\n\n\nTip\n\n\n\narrow supports many dplyr functions, but not all. If a dplyr function is not support, do as much of the work as you can with arrow, collect() the data into R, and then use the dplyr function of interest.\n\n\nLet’s say we are interested in the major drainage basin names for basin numbers “01” and “02.” We can get to this information with the following query and we only need to load a 2x2 table into R as opposed to a 2,148,847x88 table.\n\nConnect to the result data store.\nLimit the data to only the columns BASIN and BASIN_NAME and remove duplicates.\nSubset the rows to only those rows where BASIN is either “01” or “02.”\nLoad the queried data into R.\n\n\nopen_dataset(obt_result_dir) |&gt; # 1\n  distinct(BASIN, BASIN_NAME) |&gt; # 2\n  filter(BASIN %in% c(\"01\", \"02\")) |&gt; # 3\n  collect() # 4\n\n# A tibble: 2 × 2\n  BASIN BASIN_NAME                   \n  &lt;chr&gt; &lt;chr&gt;                        \n1 01    Lake Erie-Niagara River Basin\n2 02    Allegheny River Basin        \n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen performing analytics, you must be cognizant of duplicates. These data sets have been denormalized – meaning some data have been made redundant due to a one-to-many join.\nUse the dplyr functions, select() and distinct(), to help you target only the columns of interest and remove redundant rows before performing analytics.\n\n\nYou can also perform analytical summaries of the data store. Let’s say you are interested in the number of sites (SITE_CODE) that have total phosphorus data by waterbody type (WATERBODY_TYPE). The following query limits the data to the rows and columns of interest and counts the number of sties in each waterbody type.\n\nConnect to the result data store.\n\nSubset the rows to only the rows where:\n\nPARAMETER_NAME matches “phosphorus”\nFRACTION matches “total”\n\n\nKeep only the columns WATERBODY_TYPE and SITE_CODE, and remove any duplicate rows.\nAggregate by WATERBODY_TYPE and count the number of rows.\nLoad the queried data into R.\n\n\nopen_dataset(obt_result_dir) |&gt; # 1\n  filter(PARAMETER_NAME %in% \"phosphorus\", # 2\n         FRACTION %in% \"total\") |&gt; # 3\n  distinct(WATERBODY_TYPE, SITE_CODE) |&gt; # 4\n  count(WATERBODY_TYPE) |&gt; # 5\n  collect() # 6\n\n# A tibble: 4 × 2\n  WATERBODY_TYPE     n\n  &lt;chr&gt;          &lt;int&gt;\n1 lake            1349\n2 river_stream    1376\n3 other              1\n4 outfall            2\n\n\nOr maybe you are interested in finding the average and standard deviation of total phosphorus observations in each waterbody type:\n\nConnect to the result data store.\n\nSubset the rows to only the rows where:\n\nPARAMETER_NAME matches “phosphorus”\nFRACTION matches “total”\n\n\nKeep only the columns WATERBODY_TYPE, SITE_CODE, EVENT_ID, and RESULT_VALUE and remove any duplicate rows.\nAggregate by WATERBODY_TYPE.\nPerform summary statistics removing NA results (see warning note below).\nRemove the aggregate/grouping from the data.\nLoad the queried data into R.\n\n\nopen_dataset(obt_result_dir) |&gt; # 1\n  filter(PARAMETER_NAME %in% \"phosphorus\", # 2\n         FRACTION %in% \"total\") |&gt; \n  distinct(WATERBODY_TYPE, SITE_CODE, EVENT_ID, RESULT_VALUE) |&gt; # 3\n  group_by(WATERBODY_TYPE) |&gt; # 4\n  summarize( # 5\n    MEAN = mean(RESULT_VALUE, na.rm = TRUE),\n    SD = sd(RESULT_VALUE, na.rm = TRUE)\n  ) |&gt; \n  ungroup() |&gt;  # 6\n  collect() # 7\n\n# A tibble: 4 × 3\n  WATERBODY_TYPE   MEAN     SD\n  &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1 lake           0.0391  0.173\n2 river_stream   0.0601  0.149\n3 other          0.59   NA    \n4 outfall        1.39    1.46 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease note that the query above excludes non-detects using the na.rm = TRUE argument in the mean() and sd() calls. Therefore, the results are biased positively. If you are interested in a similar query, you need to decide how to handle non-detects and implement the decision prior to performing the summary statistics.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analytical Data Store</span>"
    ]
  },
  {
    "objectID": "analytical_data_store.html#common-queries",
    "href": "analytical_data_store.html#common-queries",
    "title": "\n3  Analytical Data Store\n",
    "section": "\n3.3 Common Queries",
    "text": "3.3 Common Queries\n\n3.3.1 Waterbody Data from a Specific Time Period\nWhat data were collected in Otsego Lake from 2022-2023?\n\nConnect to the result data store.\n\nSubset the rows to only the rows where:\n\nThe waterbody name is “Otsego Lake”\nThe sampling date is between 2022 and 2023\n\n\nLoad the queried data into R.\n\n\notsego_df &lt;- open_dataset(obt_result_dir) |&gt; # 1\n  filter(WATERBODY_NAME %in% \"Otsego Lake\", # 2.1\n         between(EVENT_DATETIME, # 2.2\n                 left = lubridate::ymd(\"2022-01-01\"),\n                 right = lubridate::ymd(\"2023-12-31\"))\n  ) |&gt; \n  collect() # 3",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analytical Data Store</span>"
    ]
  },
  {
    "objectID": "generate_parquet_files.html",
    "href": "generate_parquet_files.html",
    "title": "\n4  Generate Parquet Files\n",
    "section": "",
    "text": "5 Export Tables from WQMA\nQuery each of the tables from the WQMA Oracle database and save them as a parquet file to the L-drive.\npurrr::walk(names(data_model),\n            .f = function(.x) {\n              print(paste(\"Querying:\",.x))\n              tictoc::tic()\n              df &lt;- get_big_table(con = con,\n                            table = .x,\n                            n = 10000)\n              \n              arrow::write_parquet(x = df,\n                     sink = file.path(build_dir,\n                                 paste0(.x, \".parquet\")))\n              print(tictoc::toc()$callback_msg)\n              gc()\n            })\n\nDBI::dbDisconnect(con)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generate Parquet Files</span>"
    ]
  },
  {
    "objectID": "generate_parquet_files.html#duckdb",
    "href": "generate_parquet_files.html#duckdb",
    "title": "\n4  Generate Parquet Files\n",
    "section": "\n6.1 duckdb",
    "text": "6.1 duckdb\nduckdb is used to provide better memory management than would be possible if each table was loaded into R. The duckdb database created is only temporary, as indicated by dbdir = \":memorey:\". This means that once the tasks are done, the duckdb database is deleted.\n\nduckdb_con &lt;- dbConnect(duckdb(), dbdir = \":memory:\")\n\nAdd all of the parquet files in the build directory (build_dir) to the temporary duckdb database.\n\nfile_list &lt;- list.files(build_dir) |&gt; tools::file_path_sans_ext()\n\npurrr::walk(file_list,\n     ~dbSendQuery(\n       conn = duckdb_con, \n       glue(\"CREATE OR REPLACE TABLE {.x} AS \n            SELECT * FROM '{build_dir}/{.x}.parquet'\")),\n     .progress = TRUE)\n\nJoin all of the primary parent tables to the SAMPLE table and save this table as “SAMPLE_OBT”. This table is not exported as a parquet file. It is just used as the foundation for creating subsequent tables to be exported as parquet files.\n\ndbExecute(\n  conn = duckdb_con,\n  statement = \"CREATE OR REPLACE TABLE SAMPLE_OBT AS\n          SELECT * FROM EVENT\n          LEFT JOIN PROJECT USING (PROJECT)\n          LEFT JOIN SITE USING (SITE_ID)\n          LEFT JOIN BASIN USING (BASIN)\n          LEFT JOIN WATERBODY USING (WATERBODY_CODE)\n          LEFT JOIN SAMPLE USING (EVENT_ID)\"\n)\n\n\n6.1.1 obt_result\nJoin RESULT table and its parent tables (i.e., RESULT_QUALIFIER, PARAMETER, and PARAMETER_NAME) to the SAMPLE_OBT table. Exclude any rows where:\n\n\nSAMPLE_TYPE is ‘macroinverterbate_abundance’. Macroinvertebrate abundance results do not cleanly into this table. These results are stored separately (see Section 6.1.2).\n\nSAMPLE_TYPE is NULL. SQL represents empty cells as NULL, while in R these same cells would be represented as NA. It appears that there are sampling events with no sample or result data in the database. This is being investigated further. See the following issue for more details: https://github.com/BWAM/bwam_analytics/issues/7\n\nRESULT_QUALIFIER is “R”. “R” represents rejected data. These files where accidentally inserted into the database. A ticket is open with OITS to remove these records from the database. Once this is complete, this statement can be removed. See the following issue for more details: https://github.com/BWAM/data_warehouse_prep/issues/6\n\n\ndbExecute(\n  conn = duckdb_con,\n  statement = \"CREATE OR REPLACE TABLE OBT_RESULT AS\n          SELECT * FROM SAMPLE_OBT\n          LEFT JOIN RESULT USING (SAMPLE_ID)\n          LEFT JOIN RESULT_QUALIFIER USING (RESULT_QUALIFIER)\n          LEFT JOIN PARAMETER USING (PARAMETER_ID)\n          LEFT JOIN PARAMETER_NAME USING (PARAMETER_NAME)\n          WHERE SAMPLE_TYPE != 'macroinvertebrate_abundance' AND\n          SAMPLE_TYPE IS NOT NULL AND\n          RESULT_QUALIFIER != 'R'\"\n)\n\nWrite the OBT_RESULT table as parquet file, obt_result.parquet, in the analytical directory.\n\ndbExecute(\n  conn = duckdb_con,\n  statement = glue(\n    \"COPY\n          (SELECT * FROM OBT_RESULT)\n          TO '{analytical_dir}/obt_result.parquet'\n          (FORMAT 'parquet')\"\n  )\n)\n\n\n6.1.2 obt_taxa_abundance\nJoin TAXONOMIC_ABUNDANCE table and its parent tables (i.e., TAXONOMY, TAXONOMIC_TRAIT) to the SAMPLE_OBT table. Exclude any rows where:\n\n\nSAMPLE_TYPE is not ‘macroinverterbate_abundance’. This table only represents macroinvertebrate abundance results– we do not need rows representing other sample types in this table.\n\nSAMPLE_TYPE is NULL. SQL represents empty cells as NULL, while in R these same cells would be represented as NA. It appears that there are sampling events with no sample or result data in the database. This is being investigated further. See the following issue for more details: https://github.com/BWAM/bwam_analytics/issues/7\n\n\ndbExecute(\n  conn = duckdb_con,\n  statement = \"CREATE OR REPLACE TABLE OBT_TAXA_ABUNDANCE AS\n          SELECT * FROM SAMPLE_OBT\n          LEFT JOIN TAXONOMIC_ABUNDANCE USING (SAMPLE_ID)\n          LEFT JOIN TAXONOMY USING (TAXON_ID)\n          LEFT JOIN TAXONOMIC_TRAIT USING (TAXON_ID)\n          WHERE SAMPLE_TYPE = 'macroinvertebrate_abundance' AND\n          SAMPLE_TYPE IS NOT NULL\"\n)\n\nWrite the OBT_TAXA_ABUNDANCE table as parquet file, obt_taxa_abundance.parquet, in the analytical directory.\n\ndbExecute(\n  conn = duckdb_con,\n  statement = glue(\n    \"COPY\n          (SELECT * FROM OBT_TAXA_ABUNDANCE)\n          TO '{analytical_dir}/obt_taxa_abundance.parquet'\n          (FORMAT 'parquet')\"\n  )\n)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Generate Parquet Files</span>"
    ]
  },
  {
    "objectID": "data_dictionary.html",
    "href": "data_dictionary.html",
    "title": "\n5  Data Dictionary\n",
    "section": "",
    "text": "The data dictionary below describes data from the WQMA Oracle database and the WQMA Analytical Data Store (WQMA parquet files). To get this table in R, use:\n\ndata(\"columns_dictionary\", package = \"nexus\")\n\n\n\n\n\n\n\nImportant\n\n\n\nThe data dictionary empowers data users to quickly become familiar with a data set. Although the table is simple, it is critical to operations that this table remain up-to-date, accurate, and easy to understand.\nIf you find any errors or do not find the documentation clear, please open a GitHub Issue and we can collectively work towards a solution.\nThe data dictionary is stored and maintained in the nexus package.\n\n\nThe dictionary provides:\n\ntable_name: The name of the table in the WQMA Oracle Database where the column_name is stored.\ncolumn_name: The name of the column in the data store of interest. The naming convention is the same between the WQMA Oracle Database and WQMA Analytical Data Store.\ndefinition: A description of what the column represents.\nexamples: Provides examples of the data found in a column. If more than 20 unique values are represented in the column, then 20 example character strings will be provided. If 20 or less unique values are represented in the column, all 20 values are provided.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Dictionary</span>"
    ]
  }
]