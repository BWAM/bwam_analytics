---
title: "Generate Parquet Files"
---

```{r, echo=FALSE}
# TRUE = execute code in this document. This takes a long time.
# FALSE = do not execute code in this document. This just provides documentation of the process.
run_obt = FALSE
```

```{r, eval=run_obt}
library(DBI)
library(dm)
library(duckdb)
library(glue)
library(nexus)
```

```{r, eval=run_obt}
base_dir <- file.path("L:",
                     "DOW",
                     "BWAM Share",
                     "data",
                     "parquet")
build_dir <- file.path(base_dir, "build_tables")
analytical_dir <- file.path(base_dir, "analytical_table_store")
```

Connect to the WQMA Oracle database and establish the data model.

```{r, eval=run_obt}
con <- get_connected(username = "your-user-name",
                     keyring = "wqmap")

data_model <- get_data_model(con = con)
```

Query each of the tables from the WQMA Oracle database and save them as a parquet file to the L-drive.
```{r}
purrr::walk(names(data_model),
            .f = function(.x) {
              print(paste("Querying:",.x))
              
              df <- get_big_table(con = con,
                            table = .x,
                            n = 10000)
              
              arrow::write_parquet(x = df,
                     sink = file.path(build_dir,
                                 paste0(.x, ".parquet")))
              gc()
            })
```

Query the EVENT table and all of its' parent tables. Flatten all tables into a single table.

```{r, eval=run_obt}
event_dm <- data_model |> 
  dm::dm_select_tbl(
    BASIN,
    WATERBODY,
    SITE,
    EVENT,
    PROJECT
  ) |> 
  dplyr::collect()

event_df <- event_dm |> 
  dm::dm_flatten_to_tbl(.start = "EVENT",
                        .recursive = TRUE)

arrow::write_parquet(x = event_df,
                     sink = file.path(build_dir,
                                 "event.parquet"))

rm(event_dm, event_df)
gc()
```

```{r, eval=run_obt}
sample_df <- nexus::get_big_table(con,
                                  table = "SAMPLE",
                                  n = 5000)

arrow::write_parquet(x = sample_df,
                     sink = file.path(build_dir,
                                 "sample.parquet"))

rm(sample_df)
gc()
```

```{r, eval=run_obt}
result_df <- get_big_table(con, table = "RESULT", n = 20000)

arrow::write_parquet(x = result_df,
                     sink = file.path(build_dir,
                                 "result.parquet"))

rm(result_df)
gc()
```

```{r, eval=run_obt}
taxa_dm <- data_model |> 
  dm::dm_select_tbl(
    TAXONOMY,
    TAXONOMIC_TRAIT,
    TAXONOMIC_ABUNDANCE) |> 
  dplyr::collect()

taxa_df <- taxa_dm |> 
  dm::dm_flatten_to_tbl(.start = TAXONOMIC_ABUNDANCE)

arrow::write_parquet(x = taxa_df,
                     sink = file.path(build_dir,
                                "taxonomic_abundance.parquet"))

rm(taxa_dm, taxa_df)
gc()
```


```{r, eval=run_obt}
param_df <- data_model |> 
  dm::dm_select_tbl(
    PARAMETER,
    PARAMETER_NAME) |> 
  dm::dm_flatten_to_tbl(.start = PARAMETER) |> 
  dplyr::collect()

arrow::write_parquet(x = param_df,
                     sink = file.path(build_dir,
                                "parameter.parquet"))

rm(param_df)
gc()
```

```{r, eval=run_obt}
qualifier_df <- data_model$RESULT_QUALIFIER |> 
  dplyr::collect()

arrow::write_parquet(x = qualifier_df,
                     sink = file.path(build_dir,
                                "qualifier.parquet"))

rm(qualifier_df)
gc()
```


# Join and Export Tables as Parquet

## duckdb

```{r}
duckdb_con <- dbConnect(duckdb(), dbdir = ":memory:")
```

```{r}
file_list <- list.files(build_dir) |> tools::file_path_sans_ext()

purrr::walk(file_list,
     ~dbSendQuery(duckdb_con, 
                    glue("CREATE OR REPLACE TABLE {.x} AS
          SELECT * FROM 
          read_parquet('{build_dir}/{.x}.parquet')
          ")),
     .progress = TRUE)





```

```{r}
dbExecute(conn = duckdb_con,
          glue("CREATE OR REPLACE TABLE obt_taxa_abundance AS
          SELECT * FROM event
          LEFT JOIN sample ON event.event_id = sample.event_id
          LEFT JOIN taxonomic_abundance ON sample.sample_id = taxonomic_abundance.sample_id
          WHERE sample.sample_type = 'macroinvertebrate_abundance'")
)

test <- dbGetQuery(conn = duckdb_con,
                   "FROM obt_taxa_abundance")
```



```{r, eval=run_obt}


obt_sample <- obt_event |> 
  dplyr::full_join(
    sample,
    by = dplyr::join_by(EVENT_ID)
  ) |> 
  dplyr::select(
    -CREATE_DATE,
    -END_DATE,
    -UPDATE_DATE,
    -UPDATED_BY_GUID
  )


```

```{r, eval=run_obt}
param_df <- data_model |> 
  dm::dm_select_tbl(
    PARAMETER,
    PARAMETER_NAME) |> 
  dm::dm_flatten_to_tbl(.start = PARAMETER) |> 
  dplyr::collect()





obt_result <- result |> 
  dplyr::full_join(y = param_df) |> 
  dplyr::full_join(y = qualifier_df) |> 
  dplyr::select(
    -CREATE_DATE,
    -END_DATE,
    -UPDATE_DATE,
    -UPDATED_BY_GUID
  )

result_parquet_path <- file.path(obt_dir,
                                 "obt_just_result.parquet")

arrow::write_parquet(x = obt_result,
                     sink = result_parquet_path)

```






```{r, eval=run_obt}
primary_obt <- dplyr::full_join(
  x = obt_sample,
  y = obt_result
) |> 
  dplyr::filter(!SAMPLE_TYPE %in% "macroinvertebrate_abundance")

arrow::write_parquet(x = primary_obt,
                     sink = file.path(obt_dir,
                                      "obt_result.parquet"))
```


