[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BWAM Analytics",
    "section": "",
    "text": "Welcome\nThe goals of this quarto book are to:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#issues-and-edits",
    "href": "index.html#issues-and-edits",
    "title": "BWAM Analytics",
    "section": "Issues and Edits",
    "text": "Issues and Edits\nIf you identify an issue with the content and/or you would like to edit a page of this book, please select the “Edit this page” or “Report an issue” text at the top right of the page.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "wqma_oracle_db.html",
    "href": "wqma_oracle_db.html",
    "title": "1  Untitled",
    "section": "",
    "text": "2 WQMA Data Warehouse",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Untitled</span>"
    ]
  },
  {
    "objectID": "wqma_oracle_db.html#connecting-to-the-wqma-data-warehouse",
    "href": "wqma_oracle_db.html#connecting-to-the-wqma-data-warehouse",
    "title": "1  Untitled",
    "section": "2.1 Connecting to the WQMA Data Warehouse",
    "text": "2.1 Connecting to the WQMA Data Warehouse\nIf you have not already, please setup an ODBC connection to the WQMA data warehouse following the instructions in the ?sec-odbc section.\n\n2.1.1 Install nexus\nThe nexus package is available on GitHub. To install packages from GitHub, first install the package, pak.\n\ninstall.packages(\"pak\")\n\nUse pak to install the nexus package from GitHub. The code below specifies the organization (“BWAM”) and the repository (“nexus”). If this fails, try the next code chunk.\n\npak::pak(\"BWAM/nexus\")\n\nIf pak fails to install the package, try using devtools and gitcreds. The difficulty with this option is you must install a GitHub PAT. To setup a PAT, see Managing Git(Hub) Credentials.\n\ndevtools::install_github(\n  repo = \"BWAM/nexus\",\n  auth_token = gitcreds::gitcreds_get()$password)\n\n\n\n2.1.2 Get Connected\nTo work with the data warehouse, you must first connect to it. This is very similar to specifying specifying a file directory containing multiple CSV files you want to import into R. nexus provides the function, get_connected(), to simplify the process for connecting to the data warehouse. At a minimum, you must provide your data warehouse username. You will be prompted to enter your password in a pop-up window. You should never supply your password in plain text because it increases the chances you accidentally share your password with others. For example, it is very easy to accidentally share your password when committing and pushing other changes to GitHub.\n\n\nWhen you are finished querying the data warehouse, please use\n`DBI::dbDisconnect(conn = insert-your-connection-name-here)`.\nThis message is displayed once per session.\n\n\n\ncon &lt;- nexus::get_connected(username = \"ZSMITH\")\n\n\n\n2.1.3 Understanding the Data Warehouse Structure\nA primary goal of the WQMA Data Warehouse and nexus is to allow users to focus on the task at hand and NOT have to spend much energy cleaning data or learning how tables relate to one another. This section outlines a number of functions that can be used to rapidly learn about the data warehouse and it’s contents.\n\n2.1.3.1 Tables and Columns and Definitions, Oh My!\nThe DBI function, dbListTables(), can be used to get a vector of tables in a database. For the WQMA data warehouse, we also need to specify the name of the schema we want to explore.\n\nDBI::dbListTables(con, schema_name = \"WQMA_OWNER\")\n\n [1] \"TAXONOMY_REFERENCE_JUNCTION\" \"PARAMETER\"                  \n [3] \"SITE\"                        \"TAXONOMIC_TRAIT\"            \n [5] \"SAMPLE_DELIVERY_GROUP\"       \"BASIN\"                      \n [7] \"RESULT\"                      \"ACCESSIONS_TABLES\"          \n [9] \"ACCESSIONS_DATA_MAP\"         \"PREVIOUS_TAXON_NAME\"        \n[11] \"QUALITY_CONTROL\"             \"PROJECT\"                    \n[13] \"PREVIOUS_SITE_ID\"            \"ACCESSIONS_FLD_XFM\"         \n[15] \"EVENT\"                       \"SAMPLE\"                     \n[17] \"WATERBODY\"                   \"ACCESSIONS_CDE_MAP\"         \n[19] \"TAXONOMY_REFERENCE\"          \"ACCESSIONS\"                 \n[21] \"RESULT_QUALIFIER\"            \"TAXONOMIC_ABUNDANCE\"        \n[23] \"PARAMETER_NAME\"              \"TAXONOMY\"                   \n[25] \"ACCESSIONS_KEY_MAP\"         \n\n\nHowever, it is recommended that you use the nexus get_data_dictionary() function because it provides much more information than dbListTables(). Setting the argument just_tables = TRUE will return tibble (i.e., a fancy data frame) of all tables in the data warehouse and their definitions.\n\n(dictionary &lt;- nexus::get_data_dictionary(just_tables = TRUE))\n\n# A tibble: 19 × 2\n   table_name                  definition                                       \n   &lt;chr&gt;                       &lt;chr&gt;                                            \n 1 BASIN                       This table represents the 17 major drainage basi…\n 2 EVENT                       A table representing all distinct sampling event…\n 3 PARAMETER                   This table represents the distinct parameters pr…\n 4 PARAMETER_NAME              This table provides descriptions of the paramete…\n 5 PREVIOUS_SITE_ID            This table represents site IDs that were previou…\n 6 PREVIOUS_TAXON_NAME         This table represents taxonomic names that were …\n 7 PROJECT                     This table represents the monitoring projects th…\n 8 QUALITY_CONTROL             This table contains quality control results asso…\n 9 RESULT                      This table represents numeric and/or categorical…\n10 RESULT_QUALIFIER            The table provides a detailed description of res…\n11 SAMPLE                      This table represents distinct samples collected…\n12 SAMPLE_DELIVERY_GROUP       A table containing all valid Sample Delivery Gro…\n13 SITE                        This table represents the distinct monitoring lo…\n14 TAXONOMIC_ABUNDANCE         This table contains taxonomic abundances— the co…\n15 TAXONOMIC_TRAIT             This table represents taxonomic traits, such as …\n16 TAXONOMY                    This table represents the tax's observed in New …\n17 TAXONOMY_REFERENCE          This table identifies taxonomic reference materi…\n18 TAXONOMY_REFERENCE_JUNCTION This is a junction table to facilitate the relat…\n19 WATERBODY                   This table represents waterbodies in New York St…\n\n\nRunning get_data_dictionary() with all the default arguments will return all table and column definitions in the data warehouse.\n\n(dictionary &lt;- nexus::get_data_dictionary())\n\n# A tibble: 218 × 11\n   table_name column_name    examples   definition  db_data_type character_limit\n   &lt;chr&gt;      &lt;chr&gt;          &lt;list&gt;     &lt;chr&gt;       &lt;chr&gt;                  &lt;dbl&gt;\n 1 BASIN      BASIN          &lt;chr [17]&gt; A unique 2… NUMBER                    NA\n 2 BASIN      BASIN_NAME     &lt;chr [17]&gt; The names … VARCHAR2(50)              50\n 3 BASIN      &lt;NA&gt;           &lt;NULL&gt;     This table… &lt;NA&gt;                      NA\n 4 EVENT      EVENT_DATETIME &lt;chr [4]&gt;  The date a… DATETIME                  NA\n 5 EVENT      EVENT_ID       &lt;chr [10]&gt; A unique s… VARCHAR2(50)              50\n 6 EVENT      PROJECT        &lt;chr [10]&gt; A monitori… VARCHAR2(10…             100\n 7 EVENT      QAPP_ID        &lt;chr [5]&gt;  The Qualit… VARCHAR2(50)              50\n 8 EVENT      SITE_ID        &lt;dbl [10]&gt; A unique s… NUMBER                    NA\n 9 EVENT      &lt;NA&gt;           &lt;NULL&gt;     A table re… &lt;NA&gt;                      NA\n10 PARAMETER  CASRN          &lt;chr [10]&gt; A CAS Regi… VARCHAR2(50)              50\n# ℹ 208 more rows\n# ℹ 5 more variables: not_null &lt;lgl&gt;, surrogate_key &lt;lgl&gt;, primary_key &lt;chr&gt;,\n#   foreign_key &lt;chr&gt;, parent_table &lt;chr&gt;\n\n\nThe dictionary contains an examples field that provides examples of the type of data contained in each column. If there are less than 20 unique values in a column, all possible values are shown. For example, you can see all of the possible values for WATERBODY_TYPE using the following code:\n\ndictionary |&gt; \n  dplyr::filter(table_name %in% \"WATERBODY\",\n                column_name %in% \"WATERBODY_TYPE\") |&gt; \n  dplyr::pull(examples)\n\n[[1]]\n[1] \"lake\"         \"river_stream\" \"other\"        \"outfall\"     \n\n\nYou can supply a character vector of table names to the argument table_vec to subset the tibble to only the table or tables of interest. The example below, only shows the definitions for the BASIN and WATERBODY tables.\n\n(dictionary &lt;- nexus::get_data_dictionary(table_vec = c(\"BASIN\",\n                                                        \"WATERBODY\")))\n\n# A tibble: 12 × 11\n   table_name column_name       examples definition db_data_type character_limit\n   &lt;chr&gt;      &lt;chr&gt;             &lt;list&gt;   &lt;chr&gt;      &lt;chr&gt;                  &lt;dbl&gt;\n 1 BASIN      BASIN             &lt;chr&gt;    A unique … NUMBER                    NA\n 2 BASIN      BASIN_NAME        &lt;chr&gt;    The names… VARCHAR2(50)              50\n 3 BASIN      &lt;NA&gt;              &lt;NULL&gt;   This tabl… &lt;NA&gt;                      NA\n 4 WATERBODY  WATERBODY_CODE    &lt;chr&gt;    A unique … VARCHAR2(50)              50\n 5 WATERBODY  BEACH_PRESENT     &lt;chr&gt;    Identifie… VARCHAR2(15)              15\n 6 WATERBODY  FISHERIES_INDEX_… &lt;chr&gt;    This is a… VARCHAR2(75)              75\n 7 WATERBODY  PUBLIC_WATER_SUP… &lt;chr&gt;    Identifie… VARCHAR2(15)              15\n 8 WATERBODY  USGS_POND_NUMBER  &lt;chr&gt;    A unique … VARCHAR2(15)              15\n 9 WATERBODY  WATERBODY_ALTERN… &lt;chr&gt;    Some wate… VARCHAR2(10…             100\n10 WATERBODY  WATERBODY_NAME    &lt;chr&gt;    The name … VARCHAR2(10…             100\n11 WATERBODY  WATERBODY_TYPE    &lt;chr&gt;    Inidicate… VARCHAR2(50)              50\n12 WATERBODY  &lt;NA&gt;              &lt;NULL&gt;   This tabl… &lt;NA&gt;                      NA\n# ℹ 5 more variables: not_null &lt;lgl&gt;, surrogate_key &lt;lgl&gt;, primary_key &lt;chr&gt;,\n#   foreign_key &lt;chr&gt;, parent_table &lt;chr&gt;\n\n\nSimilar to table_vec, you can supply a character vector of column names to the argument column_vec to subset the tibble to only the column or columns of interest. This is useful for:\n\nGetting the definition of a column(s) of interest.\nQuickly identifying which table a column comes from.\n\nThe example below shows the definitions for two columns of interest from two different tables.\n\n(dictionary &lt;- nexus::get_data_dictionary(\n  column_vec = c(\"BASIN_NAME\",\n                 \"WATERBODY_NAME\")\n))\n\n# A tibble: 2 × 11\n  table_name column_name    examples   definition   db_data_type character_limit\n  &lt;chr&gt;      &lt;chr&gt;          &lt;list&gt;     &lt;chr&gt;        &lt;chr&gt;                  &lt;dbl&gt;\n1 BASIN      BASIN_NAME     &lt;chr [17]&gt; The names o… VARCHAR2(50)              50\n2 WATERBODY  WATERBODY_NAME &lt;chr [10]&gt; The name of… VARCHAR2(10…             100\n# ℹ 5 more variables: not_null &lt;lgl&gt;, surrogate_key &lt;lgl&gt;, primary_key &lt;chr&gt;,\n#   foreign_key &lt;chr&gt;, parent_table &lt;chr&gt;\n\n\n\n\n\n2.1.4 Get Data\n\n2.1.4.1 Single Table Queries\nTo query data from a single table you can use functions from the dplyr and DBI packages. Below I have established a remote connection to the BASIN table in the data warehouse. I have not yet pulled the data into R. You may think of this as a preview of the data that you will pull into R. As you can see, you need to provide a connection to a database, the schema, and the table name.\n\n(basin_tbl &lt;- dplyr::tbl(con,\n                        I(\"WQMA_OWNER.BASIN\")))\n\n# Source:   table&lt;WQMA_OWNER.BASIN&gt; [?? x 6]\n# Database: Oracle 19.00.0000[ZSMITH@WQMAP/]\n   BASIN BASIN_NAME  CREATE_DATE         END_DATE UPDATE_DATE\n   &lt;chr&gt; &lt;chr&gt;       &lt;dttm&gt;              &lt;dttm&gt;   &lt;dttm&gt;     \n 1 01    Lake Erie-… 2024-03-26 13:32:45 NA       NA         \n 2 02    Allegheny … 2024-03-26 13:32:45 NA       NA         \n 3 03    Lake Ontar… 2024-03-26 13:32:45 NA       NA         \n 4 04    Genesee Ri… 2024-03-26 13:32:45 NA       NA         \n 5 05    Chemung Ri… 2024-03-26 13:32:45 NA       NA         \n 6 06    Susquehann… 2024-03-26 13:32:45 NA       NA         \n 7 07    Seneca-One… 2024-03-26 13:32:45 NA       NA         \n 8 08    Black Rive… 2024-03-26 13:32:45 NA       NA         \n 9 09    St. Lawren… 2024-03-26 13:32:45 NA       NA         \n10 10    Lake Champ… 2024-03-26 13:32:45 NA       NA         \n# ℹ more rows\n# ℹ 1 more variable: UPDATED_BY_GUID &lt;chr&gt;\n\n\nTo bring the data into R, you need to use the dplyr function, collect().\n\n(basin_df &lt;- dplyr::collect(basin_tbl))\n\n# A tibble: 17 × 6\n   BASIN BASIN_NAME  CREATE_DATE         END_DATE UPDATE_DATE\n   &lt;chr&gt; &lt;chr&gt;       &lt;dttm&gt;              &lt;dttm&gt;   &lt;dttm&gt;     \n 1 01    Lake Erie-… 2024-03-26 13:32:45 NA       NA         \n 2 02    Allegheny … 2024-03-26 13:32:45 NA       NA         \n 3 03    Lake Ontar… 2024-03-26 13:32:45 NA       NA         \n 4 04    Genesee Ri… 2024-03-26 13:32:45 NA       NA         \n 5 05    Chemung Ri… 2024-03-26 13:32:45 NA       NA         \n 6 06    Susquehann… 2024-03-26 13:32:45 NA       NA         \n 7 07    Seneca-One… 2024-03-26 13:32:45 NA       NA         \n 8 08    Black Rive… 2024-03-26 13:32:45 NA       NA         \n 9 09    St. Lawren… 2024-03-26 13:32:45 NA       NA         \n10 10    Lake Champ… 2024-03-26 13:32:45 NA       NA         \n11 11    Upper Huds… 2024-03-26 13:32:45 NA       NA         \n12 12    Mohawk Riv… 2024-03-26 13:32:45 NA       NA         \n13 13    Lower Huds… 2024-03-26 13:32:45 NA       NA         \n14 14    Delaware R… 2024-03-26 13:32:45 NA       NA         \n15 15    Passaic-Ne… 2024-03-26 13:32:45 NA       NA         \n16 17    Atlantic O… 2024-03-26 13:32:45 NA       NA         \n17 16    Housatonic… 2024-03-26 13:32:45 NA       NA         \n# ℹ 1 more variable: UPDATED_BY_GUID &lt;chr&gt;\n\n\nYou can use dplyr functions to work with the data in the database. dplyr will translate these functions to SQL for you.\nThis example: 1. Uses the basin_tbl connection established before 2. Filters the BASIN column to the rows that represent the major drainage basins “03” and “07” 3. Retains only the BASIN_NAME column 4. Collects the data from the database and into R.\n\n# 1\nbasin_tbl |&gt; \n# 2\n  dplyr::filter(BASIN %in% c(\"03\", \"07\")) |&gt; \n# 3\n  dplyr::select(BASIN_NAME) |&gt; \n# 4\n  dplyr::collect()\n\n# A tibble: 2 × 1\n  BASIN_NAME                      \n  &lt;chr&gt;                           \n1 Lake Ontario & Minor Tribs.     \n2 Seneca-Oneida-Oswego River Basin\n\n\nThis example: 1. Connects to the “EVENT” table 2. filters to the rows were the EVENT_DATETIME column to only the events that occurred between July 1-7th of 2022. 3. Collects the data from the database and into R.\n\n# 1\ndplyr::tbl(con,\n           I(\"WQMA_OWNER.EVENT\")) |&gt; \n# 2\n  dplyr::filter(\n    dplyr::between(\n      x = EVENT_DATETIME,\n      left = to_date(\"2022-07-01\", \"YYYY-MM-DD HH24:MI:SS\"),\n      right = to_date(\"2022-07-07\", \"YYYY-MM-DD HH24:MI:SS\")\n    )\n  ) |&gt; \n# 3\n  dplyr::collect()\n\n# A tibble: 215 × 9\n   EVENT_ID      EVENT_DATETIME      SITE_ID PROJECT QAPP_ID CREATE_DATE        \n   &lt;chr&gt;         &lt;dttm&gt;                &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dttm&gt;             \n 1 13-FDRY-0.4_… 2022-07-06 09:15:00   27578 Undefi… Unknown 2024-03-26 14:26:14\n 2 13-STNY-0.8_… 2022-07-06 15:00:00   28025 Undefi… Unknown 2024-03-26 14:26:14\n 3 13-STNY-0.8_… 2022-07-05 15:00:00   28025 Undefi… Unknown 2024-03-26 14:26:14\n 4 13-GUNK-40.3… 2022-07-05 13:15:00   27611 PWS pr… Unknown 2024-03-26 14:26:11\n 5 13-WKLEI_T4-… 2022-07-05 15:30:00   28182 PWS pr… Unknown 2024-03-26 14:26:11\n 6 14-COUL-0.9_… 2022-07-05 10:00:00   28349 PWS pr… Unknown 2024-03-26 14:26:11\n 7 07-SXML-2.2_… 2022-07-06 11:45:00   25790 PWS pr… Unknown 2024-03-26 14:26:11\n 8 07-SXML-8.0_… 2022-07-06 10:30:00   25791 PWS pr… Unknown 2024-03-26 14:26:11\n 9 07-SXML-12.4… 2022-07-06 09:15:00   25789 PWS pr… Unknown 2024-03-26 14:26:11\n10 13-CRUM-1.5_… 2022-07-06 11:00:00   27545 PWS pr… Unknown 2024-03-26 14:26:11\n# ℹ 205 more rows\n# ℹ 3 more variables: END_DATE &lt;dttm&gt;, UPDATE_DATE &lt;dttm&gt;,\n#   UPDATED_BY_GUID &lt;chr&gt;\n\n\n\n\n2.1.4.2 Multiple Table Queries (Or Operating on the Data Model)\nIn most situations, it is recommended that you acquire data using the get_data_model() function from nexus. This function has been tailored to connect to establish a remote connection to each table in the data warehouse and define the relationships between tables.\n\n(data_model &lt;- nexus::get_data_model(con = con))\n\n── Table source ────────────────────────────────────────────────────────────────\nsrc:  Oracle 19.00.0000[ZSMITH@WQMAP/]\n── Metadata ────────────────────────────────────────────────────────────────────\nTables: `BASIN`, `EVENT`, `PARAMETER`, `PARAMETER_NAME`, `PREVIOUS_SITE_ID`, … (19 total)\nColumns: 200\nPrimary keys: 13\nForeign keys: 18\n\n\nWe can visualize the table relationships with the get_erd() function.\n\nnexus::get_erd(data_model)\n\n\n\n\n\nIf you want to collect a single table, such as BASIN, you can use the following code. The get_data_model() function hides some of the complexity of connecting to the table through dplyr, such as establishing the schema, and makes it simple to import data into R.\n\n(basin_df2 &lt;- data_model$BASIN |&gt; \n  dplyr::collect())\n\n# A tibble: 17 × 2\n   BASIN BASIN_NAME                      \n   &lt;chr&gt; &lt;chr&gt;                           \n 1 01    Lake Erie-Niagara River Basin   \n 2 02    Allegheny River Basin           \n 3 03    Lake Ontario & Minor Tribs.     \n 4 04    Genesee River Basin             \n 5 05    Chemung River Basin             \n 6 06    Susquehanna River Basin         \n 7 07    Seneca-Oneida-Oswego River Basin\n 8 08    Black River Basin               \n 9 09    St. Lawrence River Basin        \n10 10    Lake Champlain River Basin      \n11 11    Upper Hudson River Basin        \n12 12    Mohawk River Basin              \n13 13    Lower Hudson River Basin        \n14 14    Delaware River Basin            \n15 15    Passaic-Newark (Ramapo River)   \n16 16    Housatonic River Basin          \n17 17    Atlantic Ocean-Long Island Sound\n\n\nOr…\n\n(basin_df2 &lt;- data_model |&gt; \n   dm::pull_tbl(table = \"BASIN\") |&gt; \n  dplyr::collect())\n\n# A tibble: 17 × 2\n   BASIN BASIN_NAME                      \n   &lt;chr&gt; &lt;chr&gt;                           \n 1 01    Lake Erie-Niagara River Basin   \n 2 02    Allegheny River Basin           \n 3 03    Lake Ontario & Minor Tribs.     \n 4 04    Genesee River Basin             \n 5 05    Chemung River Basin             \n 6 06    Susquehanna River Basin         \n 7 07    Seneca-Oneida-Oswego River Basin\n 8 08    Black River Basin               \n 9 09    St. Lawrence River Basin        \n10 10    Lake Champlain River Basin      \n11 11    Upper Hudson River Basin        \n12 12    Mohawk River Basin              \n13 13    Lower Hudson River Basin        \n14 14    Delaware River Basin            \n15 15    Passaic-Newark (Ramapo River)   \n16 16    Housatonic River Basin          \n17 17    Atlantic Ocean-Long Island Sound\n\n\n\n\n\n2.1.5 Example Queries\nYou cannot import all data from the WQMA Data Warehouse into the R– the connection will time out or you will crash your R session. However, there is no need to read in all data to R. With DBI, dplyr, dm, and nexus you have the ability to do the following and more within the database:\n\npreview data\nfilter data\nselect only the columns of interest\njoin tables together\n\nYou should try to narrow your focus to the smallest amount of data you need for your task. Queries will return results much faster if you limit the number of rows and columns that need to be transferred from the data warehouse into R. This might mean filtering the rows by basin, waterbody, site, sampling event, project, parameter, etc. or a combination of these factors to get only the data necessary. Similarly, selecting only the tables and columns of interest will help to speed queries up.\nIf you are feeling nervous about not pulling all data in, I hope that it\nnexus is designed to work with the R-package, dm. dm stands for “data model” and it provides a number of useful functions for working with relational data and has great documentation on how to use those functions (see Tutorials, Cheatsheet, and Technical Articles on the dm website). Many of the functions are designed to both execute locally in R and to be translated into a SQL query to be executed by a relational database or data warehouse. When possible, it is recommended that you favor dm queries over dbplyr, DBI, and custom SQL queries. It is not bad to write dbplyr, DBI, and custom SQL queries, but dm provides an elegant syntax for leveraging relational data that would be very difficult and time intensive to re-implement with other tools; in other words, dm is generally more efficient because easier to write and understand.\n\n2.1.5.1 Lake Sites with Phosphorus Data Collected in the last 5-Years\n\nDefine the time period of interest as start_date and end_date\nStart with the data_model object defined previously\nFilter the WATERBODY table to only the rows where the column WATERBODY_TYPE is “lake”.\nFilter the PARAMETER table to only the rows where the column PARAMTER_NAME is “phosphorus”.\nFilter the EVENT table to only the rows where the column EVENT_DATETIME represents a date in the last 5-years.\nIn this case, we are only interested in the SITE table. It’s important to note that we can use the data model to perform queries on tables besides the SITE table that ultimately influence the rows of the SITE table returned.\nIn this case, we are only interested in keeping the columns associated with the site identifier, SITE_CODE, and the sites coordinates (LATITUDE and LONGITUDE).\nCollects the data from the database and into R.\n\n\n# 1\nstart_date &lt;- Sys.Date() - lubridate::years(5)\nend_date &lt;- Sys.Date()\n\nsite_remote &lt;- data_model |&gt; # 2\n  dm::dm_filter(\n    WATERBODY = WATERBODY_TYPE == \"lake\", # 3\n    PARAMETER = PARAMETER_NAME == \"phosphorus\", # 4\n  EVENT = dplyr::between( # 5\n    x = EVENT_DATETIME,\n    left = to_date(start_date, \"YYYY-MM-DD HH24:MI:SS\"),\n    right = to_date(end_date, \"YYYY-MM-DD HH24:MI:SS\")\n  )\n  ) |&gt;\n  dm::pull_tbl(SITE) |&gt;  # 6\n  dplyr::select( # 7\n    SITE_CODE,\n    LATITUDE,\n    LONGITUDE\n  )\n\nsite_dm &lt;- dplyr::collect(site_remote) # 8\n\n\n\n2.1.5.2 Site\n\nsite_remote &lt;- dm::dm_filter(data_model,\n                             SITE = SITE_CODE %in% c(\"07-ONON-1.0\"))\n\n\nsite_dm &lt;- dplyr::collect(site_remote)\n\n\n(single_df &lt;- site_dm |&gt; \n  dm::dm_flatten_to_tbl(.start = RESULT, .recursive = TRUE) |&gt; \n  dplyr::select(\n    EVENT_ID,\n    SITE_CODE,\n    REPLICATE,\n    SAMPLE_METHOD,\n    SAMPLE_METHOD_DESCRIPTION,\n    PARAMETER_NAME,\n    RESULT_VALUE,\n    UNIT,\n    LATITUDE,\n    LONGITUDE,\n    BASIN,\n    BASIN_NAME,\n    WATERBODY_NAME,\n    WATERBODY_TYPE\n  ))\n\n# A tibble: 1,693 × 14\n   EVENT_ID             SITE_CODE REPLICATE SAMPLE_METHOD SAMPLE_METHOD_DESCRI…¹\n   &lt;chr&gt;                &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;                 \n 1 07-ONON-1.0_1986090… 07-ONON-… 1         survey        sample_event_info_sur…\n 2 07-ONON-1.0_1986090… 07-ONON-… 1         multiprobe    unknown               \n 3 07-ONON-1.0_1986090… 07-ONON-… 1         multiprobe    unknown               \n 4 07-ONON-1.0_1986090… 07-ONON-… 1         multiprobe    unknown               \n 5 07-ONON-1.0_1986090… 07-ONON-… 1         multiprobe    unknown               \n 6 07-ONON-1.0_1986090… 07-ONON-… 1         survey        macroinvertebrate_fie…\n 7 07-ONON-1.0_1986090… 07-ONON-… 1         survey        macroinvertebrate_fie…\n 8 07-ONON-1.0_1986090… 07-ONON-… 1         survey        macroinvertebrate_fie…\n 9 07-ONON-1.0_1986090… 07-ONON-… 1         survey        macroinvertebrate_fie…\n10 07-ONON-1.0_1986090… 07-ONON-… 1         survey        macroinvertebrate_fie…\n# ℹ 1,683 more rows\n# ℹ abbreviated name: ¹​SAMPLE_METHOD_DESCRIPTION\n# ℹ 9 more variables: PARAMETER_NAME &lt;chr&gt;, RESULT_VALUE &lt;dbl&gt;, UNIT &lt;chr&gt;,\n#   LATITUDE &lt;dbl&gt;, LONGITUDE &lt;dbl&gt;, BASIN &lt;chr&gt;, BASIN_NAME &lt;chr&gt;,\n#   WATERBODY_NAME &lt;chr&gt;, WATERBODY_TYPE &lt;chr&gt;\n\n\n\n\n2.1.5.3 Project\n\nproject_remote &lt;- data_model |&gt; \n  dm::dm_filter(PROJECT = PROJECT_TYPE == \"RIBS Routine\") |&gt; \n  dm::dm_select_tbl(\n    WATERBODY,\n    SITE,\n    EVENT,\n    SAMPLE,\n    RESULT,\n    PARAMETER\n  )\n\n\nproject_dm &lt;- dplyr::collect(project_remote)\n\n\n(single_df &lt;- project_dm |&gt; \n  dm::dm_flatten_to_tbl(.start = RESULT, .recursive = TRUE) |&gt; \n  dplyr::select(\n    EVENT_ID,\n    SITE_CODE,\n    REPLICATE,\n    SAMPLE_METHOD,\n    SAMPLE_METHOD_DESCRIPTION,\n    PARAMETER_NAME,\n    RESULT_VALUE,\n    UNIT,\n    LATITUDE,\n    LONGITUDE,\n    BASIN,\n    WATERBODY_NAME,\n    WATERBODY_TYPE\n  ))\n\n# A tibble: 18,911 × 13\n   EVENT_ID             SITE_CODE REPLICATE SAMPLE_METHOD SAMPLE_METHOD_DESCRI…¹\n   &lt;chr&gt;                &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;                 \n 1 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n 2 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n 3 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n 4 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n 5 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n 6 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n 7 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n 8 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n 9 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n10 13-ROND-9.9_2018061… 13-ROND-… 1         unknown       unknown               \n# ℹ 18,901 more rows\n# ℹ abbreviated name: ¹​SAMPLE_METHOD_DESCRIPTION\n# ℹ 8 more variables: PARAMETER_NAME &lt;chr&gt;, RESULT_VALUE &lt;dbl&gt;, UNIT &lt;chr&gt;,\n#   LATITUDE &lt;dbl&gt;, LONGITUDE &lt;dbl&gt;, BASIN &lt;chr&gt;, WATERBODY_NAME &lt;chr&gt;,\n#   WATERBODY_TYPE &lt;chr&gt;\n\n\n\n\n2.1.5.4 Parameter\n\nchloride_remote &lt;- dm::dm_filter(data_model,\n                             PROJECT = PROJECT_TYPE == \"RIBS Routine\",\n                             PARAMETER = PARAMETER_NAME == \"chloride\")\n\n\nchloride_dm &lt;- dplyr::collect(chloride_remote)\n\n\n(single_df &lt;- chloride_dm |&gt; \n  dm::dm_flatten_to_tbl(.start = RESULT, .recursive = TRUE) |&gt; \n  dplyr::select(\n    EVENT_ID,\n    SITE_CODE,\n    REPLICATE,\n    SAMPLE_METHOD,\n    SAMPLE_METHOD_DESCRIPTION,\n    PARAMETER_NAME,\n    RESULT_VALUE,\n    UNIT,\n    LATITUDE,\n    LONGITUDE,\n    BASIN,\n    BASIN_NAME,\n    WATERBODY_NAME,\n    WATERBODY_TYPE\n  ))\n\n# A tibble: 623 × 14\n   EVENT_ID             SITE_CODE REPLICATE SAMPLE_METHOD SAMPLE_METHOD_DESCRI…¹\n   &lt;chr&gt;                &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;                 \n 1 01-BUFF-1.7_2018041… 01-BUFF-… 1         unknown       unknown               \n 2 01-BUFF-1.7_2018061… 01-BUFF-… 1         unknown       unknown               \n 3 01-BUFF-1.7_2018081… 01-BUFF-… 1         unknown       unknown               \n 4 01-BUFF-1.7_2018101… 01-BUFF-… 1         unknown       unknown               \n 5 01-BUFF-1.7_2019041… 01-BUFF-… 1         unknown       unknown               \n 6 01-BUFF-1.7_2019061… 01-BUFF-… 1         unknown       unknown               \n 7 01-BUFF-1.7_2019081… 01-BUFF-… 1         unknown       unknown               \n 8 01-BUFF-1.7_2019102… 01-BUFF-… 1         unknown       unknown               \n 9 01-BUFF-1.7_2020042… 01-BUFF-… 1         unknown       unknown               \n10 01-BUFF-1.7_2020061… 01-BUFF-… 1         unknown       unknown               \n# ℹ 613 more rows\n# ℹ abbreviated name: ¹​SAMPLE_METHOD_DESCRIPTION\n# ℹ 9 more variables: PARAMETER_NAME &lt;chr&gt;, RESULT_VALUE &lt;dbl&gt;, UNIT &lt;chr&gt;,\n#   LATITUDE &lt;dbl&gt;, LONGITUDE &lt;dbl&gt;, BASIN &lt;chr&gt;, BASIN_NAME &lt;chr&gt;,\n#   WATERBODY_NAME &lt;chr&gt;, WATERBODY_TYPE &lt;chr&gt;",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Untitled</span>"
    ]
  },
  {
    "objectID": "wqma_oracle_db.html#setup-oracle-odbc-connectionsec-odbc",
    "href": "wqma_oracle_db.html#setup-oracle-odbc-connectionsec-odbc",
    "title": "1  Untitled",
    "section": "2.2 Setup Oracle ODBC Connection{sec-odbc}",
    "text": "2.2 Setup Oracle ODBC Connection{sec-odbc}\n\n2.2.1 Objective\nSetup an ODBC connection to the WQMA Data Warehouse.\n\n\n2.2.2 What is an ODBC?\nODBC stands for Open Database Connectivity and, once configured accordingly, it allows the user to connect to a database from another program, such as R.\n\n\n2.2.3 Oracle’s Instant Client ODBC Driver\n\nOracle’s Instant Client ODBC software is a standalone package that offers the full functionality of the Oracle ODBC driver (except the Oracle service for Microsoft Transaction Server) with a simple install. –From: https://www.oracle.com/database/technologies/releasenote-odbc-ic.html\n\nIn other words, Oracle’s Instant Client is a set of software that needs to be installed on your computer in order for you to connect to an Oracle database.\n\n\n\n\n\n\nCaution\n\n\n\nI don’t care what Oracle says, this is anything but simple for the average user.\n\n\n\n\n\n\n\n\nNote\n\n\n\nI was directed to the Oracle Instant Client from Posit’s webpage on connecting to an Oracle database..\n\n\n\n\n2.2.4 Procedure\n\nVisit Oracle’s Instant Client ODBC Release Notes webpage\nFollow the instructions in Installing Oracle Instant Client ODBC/On Windows section. You must first install the Instant Client Basic package and then install the Instant Client ODBC package. I have copied and modified the text from the webpage to try and make the steps easier to follow.\n\n“Install the Instant Client Basic or Basic Light package, as described above.”\n\nFollow the instructions at the head of the page in the “Installing Oracle Instant Client Basic and Basic Lite” section, which boils down to Installing Oracle’s Instant Client from here.\nSelect the Instant Client for Microsoft Windows (x64).\nSelect the most recent version of Instant Client for Microsoft Windows and a table will expand.\nDownload the Basic Package to the root of your Downloads folder. You cannot download the file directly to your C-drive (C:\\).\nRight-click on the downloaded zip file in your Downloads folder, select Extract All, browse to your C-drive (C:\\), and select Extract.\n\n“Download the Instant Client ODBC package. Unzip it in the same directory as your Basic or Basic Light package.”\n\nThe ODBC package is located on the same downloads page as the Basic Package installed in step 2.1 above. If you need to navigate back to this page, follow steps in 2.1.1 - 2.1.3.\nScroll down until you see the ODBC Package– it is under the subheading: Development and Runtime - optional packages.\nInstall the ODBC package to your Downloads folder. You cannot download the file directly to your C-drive (C:\\).\nRight-click on the downloaded zip file in your Downloads folder, select Extract All, browse to your C-drive (C:\\), select the folder you created in step 2.1.5, and select Extract.\n\nThe C-drive directory, created in step 2.1, must be put on your computers PATH.\n\nIn your Windows Search Bar, look up and open “Edit the system environmental variables.”\nIn the Advanced tab of the pop-up window, click the “Environmental Variables…” button\nThis step requires admin-rights.\n\nIn the “System variables” section,:\n\nfind the existing “Path” variable\nselect the “Path” row\nclick on the “Edit…” button\n\nIn the pop-up:\n\nClick on the “New” button\nClick on the “Browse…” button\nSelect the C-drive path to the instant client directory created in step 2.1\n\n\n\nThis step requires admin-rights. “Execute odbc_install.exe from the Instant Client directory.”\n\nNavigate to the folder created in step 2.2.4.\nDouble-click the odbc_install.exe.\n\n\nSetup a 64-bit ODBC connection.\n\nIn your Windows search bar, type “ODBC Data Sources”.\nSelect the Option that ends in “64-bit”.\nIn the window that appears, select the “Add” button.\nSelect the Oracle Instant Client option (should look something like this: “Oracle in instant_client_19_19”) and click the “Finish” button. If you do not see the Oracle Instant Client Option, then your ODBC installation was not successful. Please try to install the ODBC again.\nConfiguration:\n\nData Source Name: wqma_prod\nDescription: WQMA Data Warehouse Production Version\nTNS Service Name: WQMAP\nUser ID: [Enter your database User ID]\n\nExample: JDOE\n\nClick on the “Test Connection” Button, enter your database password, and click on the “OK” button. If there are no errors, you should receive a message stating “Connection successful.”\n\nIf you are entering a temporary password, you will be prompted for a new password.\n\nClick the “OK” button in the Oracle ODBC Driver Configuration window.\n\n\nCongratulations! You have successfully downloaded, installed, and configured the necessary software to connect to an Oracle database.\n\n\n\n2.2.5 Keyring Setup (Optional)\nIf you do not want to enter your password each time you establish a connection, you can setup a keyring to securely store and access your password. Again, you should NEVER type out your password in plain text.\n\nkeyring::key_set(\"your-user-name\",\n                 keyring = \"wqma-example\")\n\n\nkeyring::key_get(\"your-user-name\",\n                 keyring = \"wqma-example\")\n\nOnce the keyring is established, you can point to the keyring in the get_connected() function using the kerying argument and you will not need to provide your password each time you connect to the WQMA data warehouse.\n\ncon &lt;- nexus::get_connected(username = \"your-user-name\",\n                            keyring = \"wqma-example\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Untitled</span>"
    ]
  },
  {
    "objectID": "analytical_data_store.html",
    "href": "analytical_data_store.html",
    "title": "2  analytical_data_store",
    "section": "",
    "text": "3 Analytical Data Store\nThe goal of Analytical Data Store is to provide a simple and fast way to access BWAM water quality data.\nData from the BWAM Oracle Data Warehouse, Water Quality Monitoring and Assessment (WQMA), have been extracted, collapsed into a two large tables, and saved as a parquet files. The two tables represent:\nWe will use the packages arrow to connect to the parquet file and dplyr to help us query data.\nLet’s load the necessary packages and establish the file path to the results parquet file. The parquet file is stored on the L-drive and should be accessible to all DOW staff.\nlibrary(arrow)\n\n\nAttaching package: 'arrow'\n\n\nThe following object is masked from 'package:utils':\n\n    timestamp\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# One Big Table Directory -------------------------------------------------\nobt_dir &lt;- file.path(\"L:\",\n                     \"DOW\",\n                     \"BWAM Share\",\n                     \"data\",\n                     \"dev\",\n                     \"parquet\",\n                     \"obt_result.parquet\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>analytical_data_store</span>"
    ]
  },
  {
    "objectID": "analytical_data_store.html#parquet-and-arrow-introduction",
    "href": "analytical_data_store.html#parquet-and-arrow-introduction",
    "title": "2  analytical_data_store",
    "section": "3.1 Parquet and Arrow Introduction",
    "text": "3.1 Parquet and Arrow Introduction\n\nApache Parquet is an open source, column-oriented data file format designed for efficient data storage and retrieval. it provides high performance compression and encoding schemes to handle complex data in bulk and is supported in many programming language and analtyics tools.\n-Parquet (apache.org)\n\nAs described above, parquet files are designed for efficient data retrieval. The data are stored in a columnar format, enabling rapid data retrievals. Using Apache Arrow, analytics can be performed in-memory (using RAM) on parquet files without the need to pull of the data into R or python. In general, this means queries can be performed very rapidly and you only load data you need into R or python.\nAdditionally, parquet files have meta data that describe the column types (character, numeric, date, etc.). Unlike, column separated values (CSVs) or excel file (XLSX), there is no need to tell R or python what the data type of data are stored in each column. This enables users to query data and begin working with it immediately with out the overhead of specifying column type details with every new analysis.\n\n\n\n\n\n\nNote\n\n\n\nFor more information about Apache Parquet or Arrow, please visit:\n\nParquet (apache.org)\nApache Arrow | Apache Arrow",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>analytical_data_store</span>"
    ]
  },
  {
    "objectID": "analytical_data_store.html#query-overview",
    "href": "analytical_data_store.html#query-overview",
    "title": "2  analytical_data_store",
    "section": "3.2 Query Overview",
    "text": "3.2 Query Overview\nParquet files can be queried using the python library, PyArrow, or the R package, arrow. The remainder of this chapter will focus on using the R package, arrow.\nQuerying data is very similar to reading CSV files with read.csv(), but it is a two step process:\n\nConnect to the parquet file using the arrow function, open_dataset()\nRead data into R, using the dplyr function, collect()\n\n\nobt &lt;- open_dataset(obt_dir) |&gt; # 1\n  collect() # 2\n\n\n\n\n\n\n\nTip\n\n\n\nAlthough it is not recommended for these data stores, you can use the arrow function, read_parquet(), to perform the above query in a single step. This is not recommended, because in almost all instances you do not need all data in a data store for your analysis.\n\n\nThe open_dataset() function can be followed by dplyr functions to query and perform analytics on the data store before bringing the data into R.\n\n\n\n\n\n\nTip\n\n\n\narrow supports many dplyr functions, but not all. If a dplyr function is not support, do as much of the work as you can with arrow, collect() the data into R, and then use the dplyr function of interest.\n\n\nLet’s say we are interested in the major drainage basin names for basin numbers “01” and “02.” We can get to this information with the following query and we only need to load a 2x2 table into R as opposed to a 2,148,847x88 table.\n\nConnect to the result data store.\nLimit the data to only the columns BASIN and BASIN_NAME and remove duplicates.\nSubset the rows to only those rows where BASIN is either “01” or “02.”\nLoad the queried data into R.\n\n\nopen_dataset(obt_dir) |&gt; # 1\n  distinct(BASIN, BASIN_NAME) |&gt; # 2\n  filter(BASIN %in% c(\"01\", \"02\")) |&gt; # 3\n  collect() # 4\n\n# A tibble: 2 × 2\n  BASIN BASIN_NAME                   \n  &lt;chr&gt; &lt;chr&gt;                        \n1 01    Lake Erie-Niagara River Basin\n2 02    Allegheny River Basin        \n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen performing analytics, you must be cognizant of duplicates. These data sets have been denormalized – meaning some data have been made redundant due to a one-to-many join.\nUse the dplyr functions, select() and distinct(), to help you target only the columns of interest and remove redundant rows before performing analytics.\n\n\nYou can also perform analytical summaries of the data store. Let’s say you are interested in the number of sites (SITE_CODE) that have total phosphorus data by waterbody type (WATERBODY_TYPE). The following query limits the data to the rows and columns of interest and counts the number of sties in each waterbody type.\n\nConnect to the result data store.\nSubset the rows to only the rows where:\n\nPARAMETER_NAME matches “phosphorus”\nFRACTION matches “total”\n\nKeep only the columns WATERBODY_TYPE and SITE_CODE, and remove any duplicate rows.\nAggregate by WATERBODY_TYPE and count the number of rows.\nLoad the queried data into R.\n\n\nopen_dataset(obt_dir) |&gt; # 1\n  filter(PARAMETER_NAME %in% \"phosphorus\", # 2\n         FRACTION %in% \"total\") |&gt; # 3\n  distinct(WATERBODY_TYPE, SITE_CODE) |&gt; # 4\n  count(WATERBODY_TYPE) |&gt; # 5\n  collect() # 6\n\n# A tibble: 4 × 2\n  WATERBODY_TYPE     n\n  &lt;chr&gt;          &lt;int&gt;\n1 lake            1363\n2 river_stream    1552\n3 outfall            2\n4 other              1\n\n\nOr maybe you are interested in finding the average and standard deviation of total phosphorus observations in each waterbody type:\n\nConnect to the result data store.\nSubset the rows to only the rows where:\n\nPARAMETER_NAME matches “phosphorus”\nFRACTION matches “total”\n\nKeep only the columns WATERBODY_TYPE, SITE_CODE, EVENT_ID, and RESULT_VALUE and remove any duplicate rows.\nAggregate by WATERBODY_TYPE.\nPerform summary statistics removing NA results (see warning note below).\nRemove the aggregate/grouping from the data.\nLoad the queried data into R.\n\n\nopen_dataset(obt_dir) |&gt; # 1\n  filter(PARAMETER_NAME %in% \"phosphorus\", # 2\n         FRACTION %in% \"total\") |&gt; \n  distinct(WATERBODY_TYPE, SITE_CODE, EVENT_ID, RESULT_VALUE) |&gt; # 3\n  group_by(WATERBODY_TYPE) |&gt; # 4\n  summarize( # 5\n    MEAN = mean(RESULT_VALUE, na.rm = TRUE),\n    SD = sd(RESULT_VALUE, na.rm = TRUE)\n  ) |&gt; \n  ungroup() |&gt;  # 6\n  collect() # 7\n\n# A tibble: 4 × 3\n  WATERBODY_TYPE   MEAN     SD\n  &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1 lake           0.0393  0.171\n2 river_stream   0.0607  0.153\n3 outfall        1.39    1.46 \n4 other          0.59   NA    \n\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease note that the query above excludes non-detects using the na.rm = TRUE argument in the mean() and sd() calls. Therefore, the results are biased positively. If you are interested in a similar query, you need to decide how to handle non-detects and implement the decision prior to performing the summary statistics.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>analytical_data_store</span>"
    ]
  },
  {
    "objectID": "analytical_data_store.html#common-queries",
    "href": "analytical_data_store.html#common-queries",
    "title": "2  analytical_data_store",
    "section": "3.3 Common Queries",
    "text": "3.3 Common Queries\n\n3.3.1 Waterbody Data from a Specific Time Period\nWhat data were collected in Otsego Lake from 2022-2023?\n\nConnect to the result data store.\nSubset the rows to only the rows where:\n\nThe waterbody name is “Otsego Lake”\nThe sampling date is between 2022 and 2023\n\nLoad the queried data into R.\n\n\notsego_df &lt;- open_dataset(obt_dir) |&gt; # 1\n  filter(WATERBODY_NAME %in% \"Otsego Lake\", # 2.1\n         between(EVENT_DATETIME, # 2.2\n                 left = lubridate::ymd(\"2022-01-01\"),\n                 right = lubridate::ymd(\"2023-12-31\"))\n  ) |&gt; \n  collect() # 3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>analytical_data_store</span>"
    ]
  }
]